{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2_model_fitting_and_model_comparison",
      "version": "0.3.2",
      "views": {},
      "default_view": {},
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "UkKEaRXlM8Ik",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## CCNSS 2018 module 2\n",
        "# Tutorial 2 - Model fitting and model comparison"
      ]
    },
    {
      "metadata": {
        "id": "drvT_JQzM8Il",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "*Please execute the cells bellow to initialize the notebook environment*"
      ]
    },
    {
      "metadata": {
        "id": "g04OfLlSSc7v",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "!if [ ! -f ddm_tutorial1.py ]; then git clone https://github.com/ccnss/ccnss2018_students; \\\n",
        "                                    cp -f ccnss2018_students/module2/2_model_fitting_and_model_comparison/ddm_tutorial1.py ./; fi"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lsdTRw8tM8Im",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt    # import matplotlib\n",
        "import numpy as np                 # import numpy\n",
        "import scipy as sp                 # import scipy\n",
        "import math                        # import basic math functions\n",
        "import random                      # import basic random number generator functions\n",
        "from scipy.optimize import minimize\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "import time\n",
        "\n",
        "fig_w, fig_h = (6, 4)\n",
        "plt.rcParams.update({'figure.figsize': (fig_w, fig_h)})\n",
        "plt.style.use('ggplot')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "N5--s4zpM8Ip",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "\n",
        "## Objectives\n",
        "\n",
        "\n",
        "In this notebook we'll look at *model fitting* using the Drift Diffusion Model (DDM) we developped in Tutorial 1. That is, we will attempt to recover the parameters that generated the data with the DDM. We will then have a look at *model comparison* and *model selection*.\n",
        "\n",
        "Model fitting:\n",
        "\n",
        "- Ordinary least squares (OLS)\n",
        "- Likelihood, and Maximum Likelihood Estimation (MLE)\n",
        "    - Grid search approximation\n",
        "    - Gradient Descent optimisation\n",
        "\n",
        "Model Comparison:\n",
        "\n",
        "- Model Selection using:\n",
        "    - Bayesian Information Criterion (BIC)\n",
        "    - Akaike Information Criterion (AIC)\n",
        "\n",
        "---\n"
      ]
    },
    {
      "metadata": {
        "id": "3g7gSjCxM8Ir",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Ordinary Least Squares (OLS) Regression\n",
        "\n",
        "We have data pairs $(x_i,y_i)$ that we think are linearly related but we are not sure what the slope or intercept is that best characterizes this relationship. To find this, we fit the data with a linear model\n",
        "\n",
        "\\begin{align*} \\hat{y}_i = \\beta_1 x_i + \\beta_0 \\end{align*}\n",
        "\n",
        "and estimate the best fitting parameters by minimizing the mean squared error (MSE). That is, we want to find the parameters $\\beta_0$ and $\\beta_1$ such that the line fits the data as best as possible. To do so, we will calculate the squared error (i.e. squared distance) between the line and the data-points, and choose the parameters that gives us the least error possible (i.e. the minimum error across all data points):\n",
        "\n",
        "\\begin{align*} \\sum_i(y_i - \\hat{y}_i)^2 = \\sum_i (y_i-\\beta_1 x_i-\\beta_0)^2 \\end{align*}\n",
        "\n",
        "For the case of linear regression, there is actually an analytical solution:\n",
        "\n",
        "\\begin{align*} \\beta_1 = \\frac{cov(x,y)}{var(x)} \\end{align*}\n",
        "\n",
        "\\begin{align*} \\beta_0 = \\bar{y} - \\hat{\\beta_1} x \\end{align*}\n",
        "\n",
        "but we will use a more general optimization library to start familiarizing ourselves with these tools."
      ]
    },
    {
      "metadata": {
        "id": "BjhniiLdM8Ir",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "** EXERCISE 1: OLS **\n",
        "\n",
        "We will generate 100 data points with a linear relationship, and attempt to recover the parameters that generated the data.\n",
        "\n",
        "** Instructions **\n",
        "* Set the seed to 0 using `np.random`\n",
        "* Generate N = 100 data pairs $(x_i,y_i)$ using a linear model with normally distributed noise $\\epsilon = 1$ and your choice of slope $\\beta_1=1$ and intercept $\\beta_0=0.5$ parameters.\n",
        "* Calculate the analytical estimate for the OLS regression. (hint: use sp.cov and sp.var)\n",
        "* Write a function that returns the mean square error (MSE) for any parameter values.\n",
        "* Use an optimization library to numerically find the parameters that minimize the MSE and compare these to the true parameters of the generative model (hint: you may want to use the function `sp.optimize.minimize(function,initial_guesses)`.\n",
        "* Plot the data, as well as the anlytical and numerical OLS estimate\n"
      ]
    },
    {
      "metadata": {
        "id": "mRP8AX8ai8YX",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "#insert your code here\n",
        "\n",
        "# Generate the data\n",
        "beta1 = \n",
        "beta0 = \n",
        "sigma = \n",
        "xdata = \n",
        "eps   = \n",
        "ydata = \n",
        "\n",
        "# Compute the analytical solutions\n",
        "beta1_est = \n",
        "beta0_est = \n",
        "print('Analytical B0: ' + str(beta0_est), ', B1: ' + str(beta1_est))\n",
        "\n",
        "# Function that returns the MSE\n",
        "def get_MSE(params):   \n",
        "    \n",
        "    return \n",
        "\n",
        "# Optimization (recovering the parameters)\n",
        "initial_guess = \n",
        "res = #use minimize\n",
        "print('Optimization B0: ' + str(res.x[0]), ', B1: ' + str(res.x[1]))\n",
        "\n",
        "# Plot the data\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_60oXnEHM8Iz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**EXPECTED OUTPUT**\n",
        "\n",
        "```\n",
        "Analytical B0: 0.5744796719087789 , B1: 1.1259580136832137\n",
        "Optimization B0: 0.5751530820156441 , B1: 1.1146984257201837\n",
        "```\n",
        "![](https://github.com/ccnss/ccnss2018_students/raw/master/module2/2_model_fitting_and_model_comparison/figures/expected_ex1.png)"
      ]
    },
    {
      "metadata": {
        "id": "Pwy7-SmxM8I0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Maximum Likelihood Estimation (MLE)\n",
        "\n",
        "We can also fit the model to the data using Maximum Likelihood Estimation methods. To do this, we take the following generative model for the data:\n",
        "\n",
        "\\begin{align*} \\hat{y}_i = \\beta_1 x_i + \\beta_0 + \\epsilon \\end{align*}\n",
        "\n",
        "where $\\epsilon\\sim \\mathcal{N}(\\mu=0,\\sigma^2)$ is a normally distributed random variable with mean 0 and variance $\\sigma^2$ and $y\\sim \\mathcal{N}(\\mu=(\\beta_1 x+\\beta_0),\\sigma^2)$.\n",
        "\n",
        "The probability distribution of $y$ given $x$ is then given by: \n",
        "\n",
        "\\begin{eqnarray}\n",
        "\\\\\n",
        "p(y_i|x_i,\\beta_1,\\beta_0) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp\\left[-(y_i-(\\beta_1 x_i-\\beta_0))^2/(2\\sigma^2)\\right]\n",
        "\\end{eqnarray}\n",
        "\n",
        "For a pair $(x_i,y_i)$, the log likelihood of observation $y_i$ is \n",
        "\\begin{eqnarray}\n",
        "\\log p(y_i|x_i,a,b)\n",
        "= \\log \\left[\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp\\left[-(y_i-(\\beta_1 x_i-\\beta_0))^2/(2\\sigma^2)\\right]\\right] \\\\\n",
        "= \\log \\left[\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\right] -(y_i-(\\beta_1 x_i-\\beta_0))^2/(2\\sigma^2)\n",
        "\\end{eqnarray}\n",
        "\n",
        "That is: When $\\epsilon$ is normally distributed, maximizing the total log likelihood of the data is equivalent to minimizing the mean squared error.\n"
      ]
    },
    {
      "metadata": {
        "id": "Drcv2rwOM8I0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "*** EXERCISE 2: MLE - Grid-Search ***\n",
        "\n",
        "Using the generated 100 data points from EXERCISE 1, calculate the Likelihood of the data given the parameters. That is:\n",
        "\n",
        "\\begin{align*} p\\left(Data_{y_i} \\mid \\beta_1,\\beta_0 \\right) \\end{align*}\n",
        "\n",
        "We will then extract the maximum of the log likelihood (or inversely, the minimum of the negative log likelihood). That is, the parameters that maximizes the chance of observing the data:\n",
        "\n",
        "\\begin{align*} p\\left(\\beta_1,\\beta_0 \\mid Data \\right) = \\arg \\max \\sum_{y_i} \\log p\\left(Data_{y_i} \\mid \\beta_1,\\beta_0 \\right)\\end{align*}\n",
        "\n",
        "*** Instructions ***\n",
        "\n",
        "* Write a function that returns the total negative of the log likelihood for any parameter values.\n",
        "* Search (loop) through parameter values ranging from -4 to 4 in increments of 0.01, calculate and store the negative log likelihood for each parameter value of the loop.\n",
        "* Print the parameters found for $\\beta_0$, and $\\beta_1$ to the true parameters.\n",
        "* Print the error between the true parameters and those found using optimization, and time the function took to compute the likelihood for all pairs of parameter values.\n",
        "* Plot the log likelihood as a function of parameter values ($\\beta_0$ and $\\beta_1$) using a heatmap, and mark the value with lowest negative log likelihood (hint: you may want to use the function `plt.imshow()` or `plt.contourf()`)\n",
        "    * (Optional) Alternatively you could try to make fancier 3D-plots using : `plt.plot_surface()`, or `plt.plot_wireframe()`)\n",
        "* (Optional) Change the step-size (coarseness) of the grid, and see how this affects your parameter estimates."
      ]
    },
    {
      "metadata": {
        "id": "qGJu3fK9jwLg",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "#insert your code here\n",
        "\n",
        "# Function that returns the negative ll (nll)\n",
        "def get_nll(params):\n",
        "    y_hat = \n",
        "    \n",
        "    # calculate the likelihood \n",
        "    likelihood = # keep likelihood > 0 by adding a small constant (e.g. 1e-100), so that we can take log\n",
        "    \n",
        "    return  \n",
        "\n",
        "# initialize your variables  \n",
        "\n",
        "# Loop through grid and save nLL at each grid point\n",
        "\n",
        "# Find indices of the grid which gives you the lowest nLL\n",
        "\n",
        "# Print the estimated B0 & B1\n",
        "print('Estimated B0: '+ str( ) + ', estimated B1: ' +str( ) + ', estimated nLL: ' +str(   ) )\n",
        "print('Rounded Error B0: ' + str(round(   ,2)) + ' , rounded error B1: ' + str(round(   ,2)) + ' , Time taken : ' + str(round(   ,2)) + ' seconds')\n",
        "\n",
        "# Plot using imshow()\n",
        "plt.figure()\n",
        "plt.title('Likelihood plot using: `imshow`')\n",
        "plt.imshow(           , origin='lower',\n",
        "            cmap='hot', extent=extent, aspect='auto')\n",
        "plt.axvline(beta1,linestyle='-.',color='k')\n",
        "plt.axhline(beta0,linestyle='-.',color='k')\n",
        "plt.plot(beta1,beta0,'ok')\n",
        "plt.xlabel(r'$\\beta_1$')\n",
        "plt.ylabel(r'$\\beta_0$')\n",
        "plt.colorbar(label='Log Likelihood')\n",
        "\n",
        "# Plot using contourf()\n",
        "plt.figure()\n",
        "plt.title('Likelihood plot using: `contourf`')     \n",
        "plt.contourf(    ,    ,     , 40, cmap=plt.cm.bone)\n",
        "plt.axvline(beta1,linestyle='-.',color='k')\n",
        "plt.axhline(beta0,linestyle='-.',color='k')\n",
        "plt.plot(beta1,beta0,'ok')\n",
        "plt.xlabel(r'$\\beta_1$')\n",
        "plt.ylabel(r'$\\beta_0$')\n",
        "plt.colorbar(label='Log Likelihood')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vjxIOlIPM8I-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "*** Expected output ***\n",
        "\n",
        "```\n",
        "Estimated B0: 0.5799999999999024, estimated B1: 1.109999999999891, estimated nLL: -144.75460963988417\n",
        "Rounded Error B0: 0.08 , rounded error B1: 0.11 , Time taken : 15.22 seconds\n",
        "```\n",
        "\n",
        "![](https://github.com/ccnss/ccnss2018_students/raw/master/module2/2_model_fitting_and_model_comparison/figures/expected_ex2_1.png)\n",
        "\n",
        "![](https://github.com/ccnss/ccnss2018_students/raw/master/module2/2_model_fitting_and_model_comparison/figures/expected_ex2_2.png)"
      ]
    },
    {
      "metadata": {
        "id": "UZDJUQWtnBZ_",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# --- OPTIONAL ---\n",
        "\n",
        "# insert your code here to complete the plotting functions\n",
        "\n",
        "# Plot using plot_surface()\n",
        "fig = plt.figure()\n",
        "plt.title('Likelihood plot using: `plot_surface`')\n",
        "ax = Axes3D(fig)\n",
        "surf = ax.plot_surface(      ,      ,      , cmap=plt.cm.coolwarm,\n",
        "                       linewidth=0, antialiased=False)\n",
        "plt.xlabel(r'$\\beta_1$')\n",
        "plt.ylabel(r'$\\beta_0$')\n",
        "ax.set_zlabel('Log Likelihood')\n",
        "\n",
        "# Plot using wire_frame()\n",
        "fig = plt.figure()\n",
        "plt.title('Likelihood plot using: `contourf`')\n",
        "ax = Axes3D(fig)\n",
        "ax.plot_wireframe(      ,      ,      , color='blue', rstride=20, cstride=20)\n",
        "plt.xlabel(r'$\\beta_1$')\n",
        "plt.ylabel(r'$\\beta_0$')\n",
        "ax.set_zlabel('Log Likelihood')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "R7vmG3S3M8JD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "*** (Optional) EXPECTED OUTPUT ***\n",
        "\n",
        "![](https://github.com/ccnss/ccnss2018_students/raw/master/module2/2_model_fitting_and_model_comparison/figures/expected_ex2_3.png)"
      ]
    },
    {
      "metadata": {
        "id": "-DiS8kucM8JD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "*** EXERCISE 3: MLE - Optimization (Gradient Descent) ***\n",
        "\n",
        "Instead of looping through the combination of all the parameter values in a loop. We will be using gradient descent (i.e. 'optimization' methods), to extract the maximum of the log likelihood (or inversely, the minimum of the negative log likelihood). That is, the parameters that maximizes the chance of observing the data:\n",
        "\n",
        "\\begin{align*} p\\left(\\beta_1,\\beta_0 \\mid Data \\right) = \\arg \\max \\sum_{y_i} \\log p\\left(Data_{y_i} \\mid \\beta_1,\\beta_0 \\right)\\end{align*}\n",
        "\n",
        "Intuitively, you can think of gradient descent optimization as follows: \n",
        "The optimization algorithm calculates the slope (i.e. derivative) of a function at a given point. In our case, we want to maximize the likelihood (i.e. find the top of the log-likelihood 'hill'). That is, the optimization function will calculates the gradient (derivative) of the log-likelihood given the parameter values, and 'ascend' the likelihood function (or 'descend' the negative log-likelihood) until it finds a derivative of 0 (it found a minima or maxima, up until convergence to some threshold)\n",
        "\n",
        "*** Instructions ***\n",
        "\n",
        "* Use an optimization library to numerically find the parameters that minimize the negative log likelihood (or equivalently, maximize the log likelihood of the data given the model). Tip: Use the function `minimize()` from the module `optimize` of Scipy. \n",
        "* Print the parameters found using the optimization function\n",
        "* Print the error between the true parameters and those found using optimization, and time the function took to converge\n",
        "* Compare the parameters found using optimization to those found with grid-search\n",
        "* Compare the time it took to find the parameters with optimization vs. grid-search\n"
      ]
    },
    {
      "metadata": {
        "id": "JyMfghZLnbir",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# insert your code here\n",
        "\n",
        "# Optimization (find parameters that minimize nll)\n",
        "initial_guess = \n",
        "res = \n",
        "\n",
        "print('Estimated B0: '+ str(  ) + ', estimated B1: ' +str(  ) + ', estimated nLL: ' +str(   ) )\n",
        "print('Rounded Error B0: ' + str(round(    ,2)) + ' , rounded error B1: ' + str(round(    ,2)) + ' , Time taken : ' + str(round(    ,8)) + ' seconds')\n",
        "print(' ')\n",
        "print('Grid-search took ' + str(   ) + ' times longer than using the `minimize` function')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "i8q2XGTmM8JJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "*** EXPECTED OUTPUT ***\n",
        "\n",
        "```\n",
        "Estimated B0: 0.5751530877738371, estimated B1: 1.1146984394529669, estimated nLL: 144.75244602999874\n",
        "Rounded Error B0: 0.08 , rounded error B1: 0.11 , Time taken : 0.00580382 seconds\n",
        " \n",
        "Grid-search took 2621.999671363431 times longer than using the `minimize` function\n",
        "```"
      ]
    },
    {
      "metadata": {
        "id": "PxldT0TlM8JK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Application: Fitting the DDM\n",
        "\n",
        "Now that we have looked at model fitting for a simple case, we can try to fit the DDM to the data from last class in order to find the best fitting mean, noise and boundary parameters.\n",
        "\n",
        "We will test our ability to recover the parameters of the model on simulated data for which we set the parameters.\n",
        "\n",
        "Once we are convinced that we can recover the parameters on simulated data."
      ]
    },
    {
      "metadata": {
        "id": "3TSQOuNjM8JL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "*** EXERCISE 4: Fit DDM to simulated data ***\n",
        "    \n",
        "*** Instructions ***\n",
        "\n",
        "* Set your seed to 0\n",
        "* Generate 5000 trials of simulated RT data using the constant bound DDM function ($\\mu = 0.0015, \\sigma = 0.05, B = 1$)\n",
        "* Plot the simulated RT data using your plot function from yesterday\n",
        "\n",
        "Tip: We will import a data simulation function `sim_DDM_constant()`, a plotting function `plot_rt_distribution()` and a function that computes the analytic DDM and returns the (RG) `analytic_ddm` from the module `ddm`. You can use this module or you can use your own based on the work from the last tutorial."
      ]
    },
    {
      "metadata": {
        "id": "BsFH-lXAjsOK",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "#insert code here"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BnEbbC_WM8Jb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "*** EXPECTED OUTPUT ***\n",
        "\n",
        "![](https://github.com/ccnss/ccnss2018_students/raw/master/module2/2_model_fitting_and_model_comparison/figures/expected_ex4.png)"
      ]
    },
    {
      "metadata": {
        "id": "2mTKEQSmM8Je",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "*** EXERCISE 5: Compute likelihood from analytic DDM ***\n",
        "\n",
        "In this exercise we will calculate the likelihood given parameters for the Drift Diffusion Model.\n",
        "\n",
        "*** Instructions ***\n",
        "\n",
        "* Implement the calculation of the DDM negative log-likelihood by completing the function below.\n",
        "* Use the function `analytic_ddm()` from the module `ddm_tutorial1` to calculate the log-likelihood for a correct trial where RT is $500ms$, $\\mu=0.0015$ and $B=1$.\n",
        "* What's the log-likelihood for an incorrect trial with otherwise identical parameters?\n",
        "* What's the analytical log-likelihood of the decision-variable trajectory from the previous exercise?"
      ]
    },
    {
      "metadata": {
        "id": "URWQvNpzj3_4",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "def get_nll_ddm(mu, sigma, B, rts, corrects):\n",
        "    '''\n",
        "    Determines the negative loglikelihood of the analytical DDM\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    mu : float\n",
        "        DDM drift rate\n",
        "    sigma : float\n",
        "        DDM standard deviation\n",
        "    B : \n",
        "        DDM boundary\n",
        "    rts : array_like of floats\n",
        "        reaction times for which the likelihood will be evaluated\n",
        "    corrects: array_like of bools, same length as rts\n",
        "        indicates for each rt if it was a correct trial\n",
        "        \n",
        "    Returns\n",
        "    -------\n",
        "    nll : float\n",
        "        negative log-likelihood\n",
        "    '''\n",
        "    # Evaluate the analytic ddm\n",
        "\n",
        "    # init log-likelihood (LL)\n",
        "    \n",
        "\n",
        "    # For each RT determine the LL from the corresponding\n",
        "    # correct or incorrect analytical RT proabability distribution\n",
        "    # and sum LLs over occurrences of the RT\n",
        "    \n",
        "    # - correct trial RTs\n",
        "    \n",
        "\n",
        "    # - incorrect trial RTs\n",
        "        \n",
        "    # return negative LL\n",
        "    return -ll"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QvVye2AiM8Ji",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "#insert code here"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HMGCiIRMM8Jl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "*** Expected Output ***\n",
        "\n",
        "```\n",
        "Log-Likelihood using analytical DDM, correct trial, 500ms: -7.400167869285413\n",
        "Log-Likelihood using analytical DDM, incorrect trial, 500ms: -8.600167869285412\n",
        "Log-Likelihood using analytical DDM, using decision-variable trajectory: -36579.05972243618\n",
        "```"
      ]
    },
    {
      "metadata": {
        "id": "6Wj63sOYM8Jm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "*** EXERCISE 6: Fit DDM to simulated reaction time data ***\n",
        "\n",
        "Once you are able to evaluate your likelihood function at various parameter values, it's time to fit the simulated data. The goal here is to pass the negative log likelihood to an optimizer that will find the parameters to minimize the total negative log likelihood.\n",
        "\n",
        "Note that optimizers tend to work better when parameters have the same order of magnitude. Also, the optimization function that we are going to use, `sp.optimize.minimize()`, requires that all parameters that are optimized over are packed into a vector and that this vector is the first argument of the objective function.\n",
        "\n",
        "- Execute the wrapper function below. It's exactly like the function `get_nll_ddm()`, except that it takes as first argument the vector $(1000 \\mu,B)$.\n",
        "\n",
        "Remember that this will mean rescaling the parameters returned by the optimizer in future exercises!"
      ]
    },
    {
      "metadata": {
        "id": "jI5kBVclM8Jo",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# parameters = (1000mu, B)\n",
        "get_nll_ddm_wrapper = lambda parameters, sigma, rts, corrects: get_nll_ddm(parameters[0]/1000., sigma, parameters[1], rts, corrects)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Bgr-j4k4M8Jr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "*** Instructions ***\n",
        "\n",
        "* Use the optimizer `minimize` on your negative log likelihood function to maximize the log likelihood of the simulated data. Again $\\sigma$ will be fixed at 0.05. \n",
        "* Is the optimization succesful? If yes, you should see \"message: 'Optimization terminated successfully.'\" in the output. If not, consider using a bounded optimization (check out the bounds input to the function and use method 'SLSQP'). $\\mu$ and $B$ should be positive.\n",
        "* Compare the simulated data with the fitted distribution. To do so, use the analytic_ddm function with the fitted parameter value."
      ]
    },
    {
      "metadata": {
        "id": "8J9CB-tekPkr",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "#insert your code here\n",
        "\n",
        "x0 =      # Initial guess\n",
        "bounds =  # Optimization bounds\n",
        "sigma =   # always fixed\n",
        "\n",
        "# The lambda expression allows us the pass different datasets\n",
        "res = minimize(        , \n",
        "               args=(sigma, rts_sim, corrects_sim),\n",
        "               method='SLSQP', x0=x0, bounds=bounds)\n",
        "ll =     # get neg log likelihood\n",
        "\n",
        "print(res.message)\n",
        "print('Loglikelihood is :' + str(round(   ,2)))\n",
        "\n",
        "# Plotting the simulated data\n",
        "bins = np.linspace(0,2500,51)\n",
        "plot_rt_distribution(rts_sim[corrects_sim==1], rts_sim[corrects_sim==0], bins)\n",
        "\n",
        "# Plotting the analytical results\n",
        "teval = np.arange(0,bins[-1],1)[1:]\n",
        "mu =  # don't forget the scaling\n",
        "b =   \n",
        "\n",
        "\n",
        "dist_cor, dist_err = analytic_ddm(   ,  ,  , teval)\n",
        "\n",
        "plt.plot(teval, dist_cor*(bins[1]-bins[0]), color = 'blue')\n",
        "plt.plot(teval,-dist_err*(bins[1]-bins[0]), color = 'red')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Iu4Zjz2xM8Ju",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "*** Expected output ***\n",
        "\n",
        "```\n",
        "Optimization terminated successfully.\n",
        "Loglikelihood is :-36565.58\n",
        "```\n",
        "\n",
        "![](https://github.com/ccnss/ccnss2018_students/raw/master/module2/2_model_fitting_and_model_comparison/figures/expected_ex6_2.png)"
      ]
    },
    {
      "metadata": {
        "id": "YH3OhPbAM8Jv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Model Comparison\n",
        "\n",
        "We will use the $BIC$ (Bayesian Information Criterion) to compare models:\n",
        "\n",
        "\\begin{align*} -2 \\log p(M|y) \\approx -2\\ln(L) + k\\ln(n) \\equiv BIC \\end{align*}\n",
        "\n",
        "where \n",
        "                            * $M$ is the model under consideration, \n",
        "                            * $L$ the likelihood for model $M$,\n",
        "                            * $y$ the observed data, \n",
        "                            * $k$ the number of free parameters, \n",
        "                            * $n$ the number of data points (observations)\n",
        "\n",
        "and the approximation holds for large $n$.\n",
        "\n",
        "The $BIC$ penalizes more complex models with more parameters. Specifically, in our context, the BIC penalizes the non-decision time model for its extra parameter.\n",
        "\n",
        "Note that a lower BIC is better and in general a difference of BIC 10 or more is good evidence for the model with the lower BIC.\n",
        "\n",
        "*** EXERCISE 7 : Calculating the BIC ***\n",
        "\n",
        "We will compute the BIC for three models (one full model-- as done in previous exercise), one where we fix the $B=0.5$ to a constant, and one where $mu=0$ is fixed (you can think of this as a Null model)\n",
        "\n",
        "*** Instructions ***\n",
        "\n",
        "* Fit the models with Compare the BIC for the three models (you can constrain parameter values by restricting the bounds of parameters to be optimized)\n",
        "* Which model has the smaller BIC (the smaller the better)?\n",
        "* Plot the BICs for each model"
      ]
    },
    {
      "metadata": {
        "id": "HSEoPMPckUiW",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "#insert code here"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wTT6xWVuM8Jz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "*** EXPECTED OUTPUT ***\n",
        "\n",
        "```\n",
        "BIC for models: \n",
        "Full Model BIC: 73148.1910964668\n",
        "Restricted B Model BIC: 73303.0401588169\n",
        "Restricted Mu Model BIC: 74322.5969652832\n",
        "```\n",
        "\n",
        "![](https://github.com/ccnss/ccnss2018_students/raw/master/module2/2_model_fitting_and_model_comparison/figures/expected_ex7_2.png)"
      ]
    }
  ]
}