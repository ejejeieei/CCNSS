{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "3_encoding_and_data_analysis",
      "version": "0.3.2",
      "views": {},
      "default_view": {},
      "provenance": [
        {
          "file_id": "1f0fDwkmvyFC_2ZPTL9vQgIm_UaUeTxAc",
          "timestamp": 1524738812951
        }
      ],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "TpnOn58EgfaE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## CCNSS 2018 Module 4: High dimensional representations and neural dynamics\n",
        "# Tutorial 3: Encoding and data analysis\n",
        "\n",
        "[source](https://colab.research.google.com/drive/1N5sbssbgyyGqW4k2XgOdJNmB7MPhmgBZ)\n"
      ]
    },
    {
      "metadata": {
        "id": "hi2UOXjigfaF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "# This is formatted as code\n",
        "```\n",
        "\n",
        "*Please execute the cells bellow in order to initialize the notebook environment*"
      ]
    },
    {
      "metadata": {
        "id": "uKm5QqvhduNc",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "!if [ ! -f homework_2_utils.py ]; then git clone https://github.com/ccnss/ccnss2018_students; \\\n",
        "                        cp -rf ccnss2018_students/module4/3_encoding/homework_2_utils.py ./; \\\n",
        "                        cp -rf ccnss2018_students/module4/3_encoding/npp.py ./; fi"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nJgJVfSugfaG",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt    # import matplotlib\n",
        "import numpy as np                 # import numpy\n",
        "import math                        # import basic math functions\n",
        "import random                      # import basic random number generator functions\n",
        "from homework_2_utils import make_data\n",
        "\n",
        "fig_w, fig_h = (6, 4)\n",
        "plt.rcParams.update({'figure.figsize': (fig_w, fig_h)})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UwXIfh1Y9j66",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Objectives\n",
        "\n",
        "In this notebook we will introduce data analysis techniques commonly used when  analyzing and interpreting fMRI data. "
      ]
    },
    {
      "metadata": {
        "id": "HU7L6JxVgfaK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Background\n",
        "\n",
        "Encoding and decoding are complementary operations: encoding uses stimuli to predict activity while decoding uses activity to predict information about the stimuli.\n",
        "\n",
        "###Encoding###\n",
        "\n",
        "Encoding models  consist of several distinct components. First is the set of stimuli (or the various\n",
        "task conditions) used in the experiment. For example, most fMRI studies use stimuli drawn from\n",
        "discrete classes such as faces or houses (Downing et al., 2006), or they\n",
        "probe discrete levels of a cognitive variable such as the allocation of\n",
        "spatial attention to several different locations (Brefczynski and DeYoe,\n",
        "1999). The second component is a set of features that describes the\n",
        "abstract relationship between stimuli and responses. The features consist of labels that reflect different levels of the\n",
        "independent variable (e.g. faces versus houses, different locations of\n",
        "attention, etc.). The third component is one or more regions of interest\n",
        "(ROI) in the brain from which voxels are selected. The final\n",
        "component is the algorithm that is actually used to estimate the\n",
        "model from the data. \n",
        "\n",
        "##Decoding##\n",
        "\n",
        "\n",
        "The linear classifier is an\n",
        "algorithm that uses patterns of activity across an array of voxels to\n",
        "discriminate between different levels of stimulus, experimental or\n",
        "task variables. Because classifiers exploit systematic differences in\n",
        "voxel selectivity within a region of interest (ROI), in principle they can\n",
        "detect information that would be missed by conventional analyses\n",
        "that involve spatial averaging. A linear classifier can be viewed as one specific and restricted form\n",
        "of a decoding model, a model that uses voxel activity to predict\n",
        "sensory, cognitive, or motor information.\n",
        "\n",
        "\n",
        "\n",
        "## Exercises##\n",
        "\n",
        "In the following exercises, we will explore linear regression through gradient descent as well ridge regression. Finally, from exercise 4 onwards, we will implement these ideas for the comparison between different hypotheses on how well they explain data. \n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "L58fdqcNgfaM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**EXERCISE 1**\n",
        "\n",
        "- The function gd_make_data below simulates a two-dimensional stimulus and a response of a system (e.g., population of neurons).\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "0Hv6VPlluUVE",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "def gd_make_data(nsamp=100, noise=0):    \n",
        "    # Generate a two dimensional stimulus (e.g two pixels) with correlations and 100 samples (e.g. points in time)\n",
        "    # First pixel data\n",
        "    np.random.seed(2)\n",
        "    x1 = np.random.randn(nsamp)\n",
        "\n",
        "    # Second pixel that is correlated with the first\n",
        "    x2 = .4 * x1 + .6 * np.random.randn(nsamp)\n",
        "\n",
        "    # Concatinate into a stimulus matrix - here rows are dimensions and columns are time points.\n",
        "    x = np.vstack([x1, x2])\n",
        "\n",
        "    ## Generate weights and the corresponding one dimensional response \n",
        "    # Set weights on each channel\n",
        "    b = np.array([1, 7])\n",
        "\n",
        "    # Make response of system - this is the output of our toy neuron\n",
        "    y = np.dot(x.T, b) + np.random.randn(nsamp) * noise\n",
        "    \n",
        "    return x, y\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DZZqT2DsgfaQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "- Extract the two variables x and y, and plot y as a function of x. Try different levels of noise (noise = 1, 2, 10). However, we will work with the noiseless case. \n"
      ]
    },
    {
      "metadata": {
        "id": "H-8vrvRRWuIQ",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "#Insert code here"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "C9Q6Q3z7WuIU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Expected output ###\n",
        "\n",
        "\n",
        "![](https://github.com/ccnss/ccnss2018_students/raw/master/module4/3_encoding/figures/Mod4T4E1.png)\n"
      ]
    },
    {
      "metadata": {
        "id": "DtVAnrjzgfaa",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**EXERCISE 2**\n",
        "\n",
        "Suppose you have only access to the observation/response given by the vector 'y' from the previous exercise (for the noise = 0 case). One way to estimate the weights beta that connect stimulus to response is using gradient descent. For this problem, the features are in the variable `x`, and the response is in the variable `y`.\n",
        "\n",
        "\n",
        "In this exercise, you will use gradient descent to solve this problem. You will do this in several steps. \n",
        "\n",
        "- First we we will define the error landscape. For this, we define a set of weights, i.e., the parameter space. We use the function np.meshgrid with two arguments for each of the dimensions of the stimulus. \n",
        "\n",
        "Calculate the squared error between the true response, y, and the estimated responses ys. Call this variable err. \n"
      ]
    },
    {
      "metadata": {
        "id": "G36BBpYzmQBD",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "## Insert your code here\n",
        "'''Hint: use the following b1, b2 = np.meshgrid(np.arange(-1, 10, .2), np.arange(-1, 10, .2))\n",
        "   to calculate the error you can concantenate b1 and b2\n",
        "'''\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nfdBc2kdCKOB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "- Plot the contour of error surface. Hint: Use the function plt.contour with arguments b1, b2, and the error you calculated, but first reshape for plotting, e.g., errfun = err.reshape(b1.shape) "
      ]
    },
    {
      "metadata": {
        "id": "si_kYyM3mNkT",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# Insert your code here"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "z0SEiu22WuIc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Expected output ###\n",
        "\n",
        "\n",
        "\n",
        "![](https://github.com/ccnss/ccnss2018_students/raw/master/module4/3_encoding/figures/Mod4T4E2.png)\n"
      ]
    },
    {
      "metadata": {
        "id": "bVlr0UvemX7i",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**EXERCISE 3**\n",
        "\n",
        "- Now we will use gradient descent on the error to estimate values of b1 and b2. For gradient descent,  we update the value of the parameters B as:\n",
        "\n",
        "\n",
        "\n",
        "$$B_{new} = B_{old}+ \\alpha(Bx-y)x/m$$\n",
        "\n",
        "where $\\alpha$ is the learning rate/ step size and m is the length (number of samples) of Y. Note that this update rule is based on the derivative (gradient) of the error with respect to B. \n",
        "\n",
        "$$Error (B) = (Bx-y)^2/(2*m)$$\n",
        "\n",
        "\n",
        "In our case, B is composed of b1 and b2. Estimate the  values of b1 and b2 for  10, 20, and 150 steps and use a step size of 0.1. Print the values of b1 and b2 for these step sizes. \n"
      ]
    },
    {
      "metadata": {
        "id": "BTZbbrnzWuIf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "- Plot the contour of the error surface and your regression path. Where is it converging to? "
      ]
    },
    {
      "metadata": {
        "id": "qAkICTCRWuIj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Expected output ###\n",
        "\n",
        "\n",
        "\n",
        "![](https://github.com/ccnss/ccnss2018_students/raw/master/module4/3_encoding/figures/Mod4T4E3.png)\n"
      ]
    },
    {
      "metadata": {
        "id": "fGu2Jb2oWuIj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "- The MSE is defined as the difference between the estimated y via your estimated weights b1 and b2 and the true value of y. Calculate and plot the mean squared error (MSE) as a function of step size.  "
      ]
    },
    {
      "metadata": {
        "id": "3Llvia4AWuIm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Expected output ###\n",
        "\n",
        "\n",
        "\n",
        "![](https://github.com/ccnss/ccnss2018_students/raw/master/module4/3_encoding/figures/Mod4T4E3b.png)\n"
      ]
    },
    {
      "metadata": {
        "id": "AHqBXjNiqHkn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**EXERCISE 4**\n",
        "\n",
        "In this exercise we will introduce the concept of linearizing feature spaces. Given a set of responses (e.g., voxels), it is apriori unclear what features of the stimuli are being encoded. \n",
        "\n",
        "Suppose you've done an experiment and measured responses to a bunch of stimuli. You've got three different hypotheses about how the stimuli might be represented in the responses. You instantiate these hypotheses as three different linearizing transforms, giving you three different sets of features that you can extract:  X1,  X2, and  X3 (in variables called x1, x2, and x3).\n",
        "Feature space  X1 has 12 features,  X2 has 50 features, and  X3 has 100 features.\n",
        "Note that you've recorded  m = 35 different responses (i.e.  Y is an  n×m matrix). Think of these as m different neurons or m different voxels.\n"
      ]
    },
    {
      "metadata": {
        "id": "SKVZDeWyWuIn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "First execute the cell below that will simulate the data. Print the shapes of the variables x1, x2, x3 and Y for both training and test. We will initally work with the training set. "
      ]
    },
    {
      "metadata": {
        "id": "4c_Qzioh75UN",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "from homework_2_utils import make_data\n",
        "\n",
        "num_training = 500 # total number of datapoints in training set\n",
        "num_test = 100 # total number of datapoints in test set\n",
        "num_features = [12, 50, 100] # number of features in each feature space\n",
        "num_responses = 35 # number of responses (voxels or neurons)\n",
        "\n",
        "# This is just a bunch of constants. don't worry about what they mean for now\n",
        "combs = [[0, 3, 4, 6],\n",
        "         [1, 3, 5, 6],\n",
        "         [2, 4, 5, 6]]\n",
        "true_variances = np.array([300, 0, 1500, 250, 250, 4000, 500]).astype(float)\n",
        "total_variance = 0.3\n",
        "true_variances = true_variances / true_variances.sum() * total_variance\n",
        "noise_variance = 1 - true_variances.sum()\n",
        "P_parts = [3] * 7\n",
        "Pnoise_models = [P - np.array(P_parts)[c].sum() for P,c in zip(num_features, combs)]\n",
        "\n",
        "# Generate the data!\n",
        "[x1_total, x2_total, x3_total], Y_total = make_data(num_training, num_test, P_parts, \n",
        "                                                    num_responses, true_variances, \n",
        "                                                    noise_variance, combs, Pnoise_models, \n",
        "                                                    num_features)\n",
        "\n",
        "x1 = x1_total.T[:num_training]\n",
        "x2 = x2_total.T[:num_training]\n",
        "x3 = x3_total.T[:num_training]\n",
        "Y = Y_total[:num_training]\n",
        "\n",
        "x1_test = x1_total.T[num_training:]\n",
        "x2_test = x2_total.T[num_training:]\n",
        "x3_test = x3_total.T[num_training:]\n",
        "Y_test = Y_total[num_training:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dbR4qwJyWuIr",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "print('x1 shape:', x1.shape)\n",
        "print('x2 shape:', x2.shape)\n",
        "print('x3 shape:', x3.shape)\n",
        "print('Y shape:', Y.shape)\n",
        "\n",
        "print('x1_test shape:', x1_test.shape)\n",
        "print('x2_test shape:', x2_test.shape)\n",
        "print('x3_test shape:', x3_test.shape)\n",
        "print('Y_test shape:', Y_test.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QltXvBttWuIq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Expected output ###\n",
        "```\n",
        "x1 shape: (500, 12)\n",
        "x2 shape: (500, 50)\n",
        "x3 shape: (500, 100)\n",
        "Y shape: (500, 35)\n",
        "x1_test shape: (100, 12)\n",
        "x2_test shape: (100, 50)\n",
        "x3_test shape: (100, 100)\n",
        "Y_test shape: (100, 35)\n",
        "```\n"
      ]
    },
    {
      "metadata": {
        "id": "rEYrLJTgWlWb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Model Comparison\n",
        "\n",
        "In these exercises we will use ridge regression to fit models and cross-validation to choose the best model in terms of how well it explains data and how well it generalizes to new data. \n"
      ]
    },
    {
      "metadata": {
        "id": "qTGvGlzJXwVI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Exercise 5###\n",
        "\n",
        "Ridge regression is implemented as adding the ridge component ($\\lambda B^2$) to the error function, which amounts to adding a linear term  $\\lambda B$ to the update equations. \n",
        "\n",
        "Using the training set, create a function ridge_reg that calculates regression parameters for the first response (Y[:,0]) given the hypothesis x1 and a value of the ridge parameter lambda. \n",
        "Print the estimated regression parameters for three values of lambda: lambda = 0, 30, 500 and plot them on top of each other. \n",
        "\n",
        "What happens as lambda increases? Suggestion: Modify the function created for linear regression. To account for the offset of the regression parameters (b_0), you will need to insert a column of ones to your feature space x1. Hint: use the function np.insert\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "MaK4NPzeZUJJ",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "def ridge_reg(lamb, matrix_X,output,steps, eps):\n",
        "    #steps = 1000\n",
        "    #eps = 0.01\n",
        "    b_new = np.zeros((steps,np.shape(matrix_X)[1]+1))\n",
        "    b_ridge = np.zeros(np.shape(matrix_X)[1]+1)\n",
        "    \n",
        "    newx = matrix_X\n",
        "    newx = np.insert(newx,0,1.0,axis=1)\n",
        "    inner = np.dot(newx,b_ridge)-output\n",
        "    num = (len(output))\n",
        "    \n",
        "    for ii in np.arange(1,steps):\n",
        "        deriv1 = 1/num*(np.matmul(newx.T,inner) + lamb*b_ridge)\n",
        "        \n",
        "        b_new[ii,:] = b_new[ii-1,:]-eps*deriv1#np.array([b1[ii],b2[ii]])\n",
        "        inner = np.dot(newx,b_ridge)-output\n",
        "        b_ridge = b_new[ii,:]\n",
        "        error = 1/(2*num)*(np.matmul(inner.T,inner)+lamb*np.matmul(b_ridge.T,b_ridge))\n",
        "    return b_ridge, error"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8hxoCFA5ceQR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Expected output ###\n",
        "\n",
        "```\n",
        "b_est = [ 0.00917125  0.01413807 -0.16057111 -0.06141426  0.17318263  0.11684469\n",
        "  0.31305861  0.25650786  0.18476965  0.09313252  0.17000382 -0.0114288\n",
        "  0.12844927]\n",
        "\n",
        "b_est = [ 0.00880908  0.01684649 -0.12671214 -0.04726177  0.11554028  0.09028986\n",
        "  0.25513857  0.19764594  0.1436277   0.0685549   0.14035239 -0.02226684\n",
        "  0.09317462]\n",
        "\n",
        "b_est = [ 4.19229059e-03  1.64933884e-02 -4.32852182e-02 -2.29907184e-02\n",
        "  8.43419791e-05  1.44051549e-02  7.62944092e-02  4.06679770e-02\n",
        "  4.67089236e-02  1.63980014e-02  3.95663161e-02 -4.01506593e-02\n",
        "  5.06377314e-03] ```\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "![](https://github.com/ccnss/ccnss2018_students/raw/master/module4/3_encoding/figures/Mod4T4E4.png)\n"
      ]
    },
    {
      "metadata": {
        "id": "tkuhJWeryXl-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "- Calculate the regression parameters for the same values of lambda via the closed form solution:\n",
        "$$B_{\\rm ridge} = (X^TX+\\lambda I_p)^{-1}X^Ty$$\n",
        "and check that they give you(approximately) the same solution as the solution from the gradient descent.\n"
      ]
    },
    {
      "metadata": {
        "id": "5GhiyZ8teAi-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Expected output ####\n",
        "\n",
        "```\n",
        "b_est = [ 0.00917125  0.01413807 -0.16057111 -0.06141426  0.17318263  0.11684469\n",
        "  0.31305861  0.25650786  0.18476965  0.09313252  0.17000382 -0.0114288\n",
        "  0.12844927]\n",
        "b_anal = (array([ 0.00946298,  0.03213538, -0.26416431, -0.156116  ,  0.36019483,\n",
        "        0.04595873,  0.21284559,  0.2900416 ,  0.19908108,  0.12238487,\n",
        "        0.06759392, -0.10062252,  0.0968257 ]), 0.475917092515256)\n",
        "b_est = [ 0.00880908  0.01684649 -0.12671214 -0.04726177  0.11554028  0.09028986\n",
        "  0.25513857  0.19764594  0.1436277   0.0685549   0.14035239 -0.02226684\n",
        "  0.09317462]\n",
        "b_anal = (array([ 0.00880952,  0.0168734 , -0.12686746, -0.04740365,  0.11582068,\n",
        "        0.09018392,  0.25498894,  0.19769655,  0.14364944,  0.06859888,\n",
        "        0.14019926, -0.02240027,  0.09312754]), 0.48361980985540487)\n",
        "b_est = [ 4.19229059e-03  1.64933884e-02 -4.32852182e-02 -2.29907184e-02\n",
        "  8.43419791e-05  1.44051549e-02  7.62944092e-02  4.06679770e-02\n",
        "  4.67089236e-02  1.63980014e-02  3.95663161e-02 -4.01506593e-02\n",
        "  5.06377314e-03]\n",
        "b_anal = (array([ 4.19229059e-03,  1.64933884e-02, -4.32852182e-02, -2.29907184e-02,\n",
        "        8.43419791e-05,  1.44051549e-02,  7.62944092e-02,  4.06679770e-02,\n",
        "        4.67089236e-02,  1.63980014e-02,  3.95663161e-02, -4.01506593e-02,\n",
        "        5.06377314e-03]), 0.5073323994783477)\n",
        "        ```"
      ]
    },
    {
      "metadata": {
        "id": "55pqpZwZZwIi",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Exercise 6###\n",
        "\n",
        "\n",
        "In the previous exercise we explored the concept of ridge regression for one response. In this exercise we will use ridge regression to find an optimal value of lambda for each of the three spaces (hypotheses) for the whole training set. We will do this in a series of steps:\n",
        "\n",
        "First we will divide the training part of the data set (500 training samples) into k samples of equal size. \n",
        "\n",
        "- Write a function that returns k data subsets of the original data set. This means that this function would transform  the input x1 into an array of dimensions: (k,500/k,12) and transform Y into an array of dimensions (k,500/k,35). Print these shapes (for the input x and the output y). Hint: You could use the function np.random.choice(array, shape) with arguments array (1-D array of indices) and shape (k times the size of each subset).\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "JPwzKme4oTwi",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "## Insert code here\n",
        "def k_subsets(n_folds, x, y)\n",
        "    '''\n",
        "    n_folds: the number of folds\n",
        "    x: input hypothesis\n",
        "    y: output\n",
        "    \n",
        "    returns kx and ky, which arrays of dimensions (k,500/k,12) (k,500,k,35)\n",
        "    \n",
        "    '''\n",
        "    np.random.seed(30)\n",
        "    k_x = np.zeros((k,int(lenlist/k),featsize)) ### lenlist is the number of samples (500), featsize is the number of features: 12\n",
        "    k_y = np.zeros((k,int(lenlist/k),leny))  ### leny is the number of responses(35)\n",
        "  ## Insert code here\n",
        "  \n",
        "    return kx, ky\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7tEhUMs0fRd4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Expected output ####\n",
        "```kxshape = (5, 100, 12) kyshape = (5, 100, 35)\n",
        "```\n"
      ]
    },
    {
      "metadata": {
        "id": "VKiGEfAiK1zF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "- second, we choose one of the k subsets (or \"folds\") as our validation set (within training) and the remaining k-1 folds as our training set. How many validation-training sets are there? The objective is to find the optimal value of lambda for each hypothesis. \n",
        "\n",
        "For each value of i = 1, 2, . .. k, we loop over the different lambdas and we 1) perform ridge regression (obtain parameters B) using the training set composed of the k-1 folds and 2) test our obtained parameters on the fold k. For each test, we calculate a MSE as a function of lambda. Note that there are k tests in total. Since there are m = 35 responses, we average over these responses. \n",
        "\n",
        "Write a function get_MSE that, for a given value of lambda, loops over the k different training sets and performs a ridge regression. The output of the function is the matrix of parameters B. Use the analytical form of your ridge regression function to estimate the parameters B. Moreover,  use lambdas = np.logspace(1,7,15)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "jSN24j9gqvVW",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "###Insert code. This is just a suggested structure\n",
        "def get_MSE(train_input, train_output, nfolds, model):\n",
        "    lambdas = np.logspace(1,7,15)\n",
        "    \n",
        "    \n",
        "    error_array = np.zeros((nfolds,len(lambdas))) \n",
        "    \n",
        "    for k_index in range(nfolds):\n",
        "       \n",
        "        \n",
        "        for lam_index, lam in enumerate(lambdas):\n",
        "            b_ridge = \n",
        "            error_array[k_index,lam_index] = error\n",
        "    return error_array"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QTVi1Bo1WuJL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "plots the average MSE as a function of different values of lambda."
      ]
    },
    {
      "metadata": {
        "id": "P6MqB--ofiIa",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Expected output ###\n",
        "\n",
        "\n",
        "![](https://github.com/ccnss/ccnss2018_students/raw/master/module4/3_encoding/figures/Mod4T4E5.png)\n"
      ]
    },
    {
      "metadata": {
        "id": "TXLF7qIKhryF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "- You can now loop over the different hypotheses and caculate the MSE vs lambda plot for each hypothesis x1, x2, x3. "
      ]
    },
    {
      "metadata": {
        "id": "agWKRL8fmIAU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        " ### Expected output ###\n",
        " \n",
        "![](https://github.com/ccnss/ccnss2018_students/raw/master/module4/3_encoding/figures/Mod4T4E6.png)\n"
      ]
    },
    {
      "metadata": {
        "id": "eg0BdLbtWuJY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "- From the plots you have, we can calculate the optimal lambda.\n"
      ]
    },
    {
      "metadata": {
        "id": "ENJVCt5zg6KR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Expected output ####\n",
        "\n",
        "```\n",
        "lambda1 = 517.9474679231213 lambda2 =  1389.4954943731375 lambda3  = 26826.957952797275\n",
        "\n",
        "```"
      ]
    },
    {
      "metadata": {
        "id": "NShkhXLsgfIv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "Use the optimal lambda for each model to fit the test data (the data with the 100 samples). Calculate the MSE for each model? Which is the best model?"
      ]
    },
    {
      "metadata": {
        "id": "qVjeyJ3wgPdd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Expected output ###\n",
        "```\n",
        "error1 = 0.9540706491596064 error2 = 0.8712352038422592 error3 = 0.8664830485523201\n",
        "\n",
        "```\n"
      ]
    }
  ]
}