{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "4_perceptron_learning",
      "version": "0.3.2",
      "views": {},
      "default_view": {},
      "provenance": [
        {
          "file_id": "1f0fDwkmvyFC_2ZPTL9vQgIm_UaUeTxAc",
          "timestamp": 1524738812951
        }
      ],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "TpnOn58EgfaE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## CCNSS 2018 Module 4: High dimensional representations and neural dynamics\n",
        "# Tutorial 4: Perceptron learning\n",
        "\n",
        "[source](https://colab.research.google.com/drive/1V4PFwp0qYizOEjh1n68ZYh7CxJPe_7PQ)"
      ]
    },
    {
      "metadata": {
        "id": "hi2UOXjigfaF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "*Please execute the cell bellow in order to initialize the notebook environment*"
      ]
    },
    {
      "metadata": {
        "id": "nJgJVfSugfaG",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "outputId": "4a3afe21-b462-41fc-a8aa-8722bd63a5a4",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1531370294431,
          "user_tz": -480,
          "elapsed": 899,
          "user": {
            "displayName": "Jorge Jaramillo",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "101864563573406465890"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt    # import matplotlib\n",
        "import numpy as np                 # import numpy\n",
        "import math                        # import basic math functions\n",
        "import random                      # import basic random number generator functions\n",
        "\n",
        "fig_w, fig_h = (6, 4)\n",
        "plt.rcParams.update({'figure.figsize': (fig_w, fig_h)})"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UwXIfh1Y9j66",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Objectives\n",
        "\n",
        "In this notebook we will implement the perceptron learning algorithm and introduce the concept of gradient descent for extensions to multiple layers. "
      ]
    },
    {
      "metadata": {
        "id": "HU7L6JxVgfaK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Background\n",
        "\n",
        "The perceptron belongs to the category of supervised learning algorithms, single-layer binary linear classifiers to be more specific. A typical task that can be accomplished by the perceptron is to to predict to which of two possible categories a certain data point belongs based on a set of input variables.\n",
        "\n",
        "The perceptron can be thought of an artificial neuron that receives an input pattern from N other neurons and converting that input pattern to a single output value (e.g., a single binary value.) The most important elements of the perceptron are thus: \n",
        "\n",
        "1. The input pattern, which is represented by $\\vec{x^{\\mu}}\\in\\mathbb{R}^N$ (N is the number of connections and $\\mu$ indexes the pattern) \n",
        "2. The connection weights from the N inputs, represented by a  a vector of real-valued weights $\\vec{J}\\in\\mathbb{R}^N$, \n",
        "3. An output value that depends on the input pattern and weights as \n",
        "\n",
        "$$\n",
        "g(\\vec{x}) = \\begin{cases}\n",
        "                1 & \\text{if }\\vec{J} \\cdot \\vec{x}+b> 0\\,,\\\\\n",
        "                0 & \\text{otherwise};\n",
        "             \\end{cases}\n",
        "$$\n",
        "\n",
        "where $g$ is a step function, $\\vec{J} \\cdot \\vec{x}$ is  $\\sum_{i=1}^N J_i x_i$ and b is a threshold. The learning algorithm refers to the updating of the weights $\\vec{J}$ based on target ouput values $y^{\\mu}$ and actual output values $y$.\n",
        "\n",
        "We update the weights and threshold using the perceptron rule:\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "J_i & = J_i + \\alpha (y - \\hat{y}) x_{i} \\,,\\  i=1,\\ldots, N\\,;\\\\\n",
        "b & = b + \\alpha (y - \\hat{y})\\,\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "where $$\\hat{y} = g\\left(\\vec{J}\\cdot\\vec{x} + b\\right).$$\n",
        "\n",
        "and $\\alpha\\in\\left(0,1\\right]$ is the learning rate. \n",
        "\n",
        "In this notebook we will illustrate how to train a simple network and introduce the concept of linear separability. In exercises 1 and 2 we will train a simple perceptron to represent the AND function, which takes two binary input values (1 and 0) and ouputs a 1 only if both input values are 1. \n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "L58fdqcNgfaM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**EXERCISE 1**\n",
        "\n",
        "Given a perceptron with two inputs (n =2), there are $2^2$ possible patterns: (0,0), (0,1), (1,0), (1,1). \n",
        "- For the AND function: list the possible patterns (inputs) and plot the inputs in the x-y plane, so that both outputs are distinguished (in red and blue, for example)\n",
        "- plot the line that separates the inputs that map onto \"1\" from inputs that map onto \"0\"\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "fXvhE2b7_-oc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "- find values for $\\vec{J}$ and $b$ by inspection that solves the AND problem (there is more than one solution!) \n"
      ]
    },
    {
      "metadata": {
        "id": "LC56amWvgfaZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**EXERCISE 2**\n"
      ]
    },
    {
      "metadata": {
        "id": "DtVAnrjzgfaa",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We will now, instead of finding weights by inspection, assume that the weights are random and use the learning algorithm to learn the weights. \n",
        "\n",
        "- using the function np.random.random(), initialize the vector weights randomly on the interval [0,1]"
      ]
    },
    {
      "metadata": {
        "id": "qBF6OrXJHyjR",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "bdfa31a9-6e6b-4a3f-e3e2-8e8ae5c11b90",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1531370353538,
          "user_tz": -480,
          "elapsed": 1042,
          "user": {
            "displayName": "Jorge Jaramillo",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "101864563573406465890"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "#insert code here \n",
        "np.random.seed(1)\n",
        "J_ini = np.random.random(2)\n",
        "b_ini = 0\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "metadata": {
        "id": "L3ne1sdJx6yz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Expected output ###\n",
        "```\n",
        "[0.417022   0.72032449]\n",
        "```"
      ]
    },
    {
      "metadata": {
        "id": "HYw3m4g_bQOA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We will establish the training algorithm in a few steps:\n",
        " \n",
        " - Define the output target values and the learning rate. Use $\\alpha = 0.1$\n",
        " - First, build a function \"g_step()\" with the properties described in the background above, i..e, a step function.\n",
        " - Define a function error () that calculates the difference between the target (desired output) and the real output (dependent on the weights), and then is multiplied by $\\alpha$.\n",
        " \n"
      ]
    },
    {
      "metadata": {
        "id": "wawbL6gJysxf",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "## insert code here\n",
        "\n",
        "def g(arguments):\n",
        "   '''\n",
        "   arguments: these are the inputs to your activation function.\n",
        "   the function retunrs either a 1 or a zero depending on the arguments\n",
        "   '''\n",
        "def get_error ():\n",
        "    return error\n",
        "\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0PblxTQSBRZ3",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "23958d39-ee09-4443-9c8b-7e06f5716454",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1531324253575,
          "user_tz": -480,
          "elapsed": 944,
          "user": {
            "displayName": "Jorge Jaramillo",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "101864563573406465890"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "J_ini"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.417022  , 0.72032449])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "metadata": {
        "id": "T3d3kSb4AKGG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        " - define a function(s) percep_train () that takes as inputs the initial weights $\\vec{J}$ and initial threshold $b$, and calls the function error to output new weights via the update rule. Hint: You may define a list of input-output pairs for the function AND and use that as an input. There are many ways of doing this.\n",
        " "
      ]
    },
    {
      "metadata": {
        "id": "iTsfUUelzVme",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "#insert code here\n",
        "def percep_train(weights_ini,b_ini,inputs):\n",
        "  '''\n",
        "  weights_ini: This is a vector of your (random) initial weights\n",
        "  b_ini: This is your bias or threshold, it was initialized at 0\n",
        "  inputs: This is a set of your input-output pairs that you will use for training.\n",
        "  The function returns new weights and b based on the update rule (perceptron learning rule)'''"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bfek8_iz0dd5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "- Train your perceptron for 20, 60, and 150 steps. Using your learned weights and bias b, plot the line that separates the points (the line that defines the AND function). Hint: Loop over your perceptron training algorithm N times. "
      ]
    },
    {
      "metadata": {
        "id": "UQQSFjtY1VXb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Expected output ###\n",
        "\n",
        "\n",
        "![](https://github.com/ccnss/ccnss2018_students/raw/master/module4/4_perceptron/figures/Mod4T4E2.png)\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "zgLmi_tI7l4w",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "- Do the same training for the function OR. There are many solutions, but to compare to expected output, initialize your random seed as before. "
      ]
    },
    {
      "metadata": {
        "id": "zudyFY7C1jTG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Expected output ###\n",
        "\n",
        "\n",
        "![](https://github.com/ccnss/ccnss2018_students/raw/master/module4/4_perceptron/figures/Mod4T4E2b.png)\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "x0HFWea8k4bV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**EXERCISE 3** (the XOR problem)\n",
        "\n",
        "The XOR function is a binary function whose output is \"1\" only if exactly one of the inputs is 1, and the output is \"0\" otherwise.\n",
        "\n",
        "- plot the possible input patterns for the XOR function (with a different color for different ouputs). Can you find a line that can separate the inputs? (try inspection and training)"
      ]
    },
    {
      "metadata": {
        "id": "vviAUKhUZ6yG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "- try training a network to achieve XOR. How would you solve the problem? (Try pen and paper. Hint: How many lines would you need to separate the input patterns? What does this imply for the number of layers required?)"
      ]
    },
    {
      "metadata": {
        "id": "p3Xa2Oy7-kG0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Exercise 4 ###\n",
        "\n",
        "In this exercise we will perform linear regression using the perceptron. The data is composed of 8 points in two dimensional space. To perform linear regression, you have to change (or extend) your activation function g so that the output is linear, i.e., $g(x) = x$\n",
        "\n",
        "- First, plot the data."
      ]
    },
    {
      "metadata": {
        "id": "HfXy3_hom9O4",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "x_1 = np.array([0,2.7])\n",
        "x_2 = np.array([2,3.6])\n",
        "x_3 = np.array([3.2,5])\n",
        "x_4 = np.array([2.3,4.5])\n",
        "x_5 = np.array([3.2,4])\n",
        "x_6 = np.array([0.3,3.3])\n",
        "x_7 = np.array([-0.4, 2.3])\n",
        "x_8 = np.array([1.3, 4.5])\n",
        "\n",
        "#Insert code here\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "G90dBYJk2Xxv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "- Train your perceptron to find the weights and b for linear regression. Plot the line using your new weights and b. Optional: Check your results with the built in function np.linalg.lstsq() (least squares)"
      ]
    },
    {
      "metadata": {
        "id": "XQZLF1-bBg9O",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "#insert code here\n",
        "np.random.seed(5)\n",
        "J_ini = np.random.random(1)\n",
        "b_ini = 0\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YRZdUh3e3okE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Expected output ###\n",
        "\n",
        "\n",
        "![](https://github.com/ccnss/ccnss2018_students/raw/master/module4/4_perceptron/figures/Mod4T4E4.png)\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "IAWBcvKmm5x1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**EXERCISE 5** \n",
        "\n",
        "(pen and paper) In the previous exercises we used a simple update learning rule. More generally, and closer to current implementations of perceptrons (e.g., multilayer perceptrons, deep convolutional networks), we define a cost function and use gradient descent to obtain the update rule for a general (and differentiable) activation function $g$.\n",
        "\n",
        "- Write the cost function $E(W)$ as the error for a single -layer perceptron with target $y_T$ Hint: Use the expression for $\\hat{y}$ introduced in the background. \n",
        "\n",
        "\n",
        "- Use the cost function defined above, calculate $\\frac{\\partial E(W)}{\\partial W_i}$ and  $\\frac{\\partial E(W)}{\\partial b}$  in terms of the derivative of the activation function $g$. From this derivative, write a new update rule.\n",
        "\n",
        " \n",
        " - If $g$ is a sigmoid function described by $g(a) = \\frac{1}{1+e^a}$, where a is the activation, calculate the update rule in terms of g. Hint: You can write the derivative of g in terms of g itself.  \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "UrMoDF-nFPo9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Done? Ask me for further bonus questions!"
      ]
    }
  ]
}