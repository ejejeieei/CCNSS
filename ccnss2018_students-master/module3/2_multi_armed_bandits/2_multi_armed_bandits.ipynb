{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2018_Module3_tutorial2_Multi_armed_bandits.ipynb",
      "version": "0.3.2",
      "views": {},
      "default_view": {},
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "YXlkhKsvUbNl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## CCNSS 2018 Module 3: Reinforcement learning and planning\n",
        "# Tutorial 2: Multi-armed bandits\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "fuoILifApUZ5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# **Introduction**\n",
        "\n",
        "In this tutorial, we will learn about different ways to select actions based on value estimates in an k-armed bandit context."
      ]
    },
    {
      "metadata": {
        "id": "DLdzCd5vp5CU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Environment:**\n",
        "\n",
        "Consider the following learning problem. You are faced repeatedly with a choice among $k$ different options, or actions. After each choice you receive a numerical reward chosen from a stationary probability distribution that depends on the action you selected. Your objective is to maximize the expected total reward over some time period, for example, over 1000 action selections, or time steps.\n",
        "This is the original form of the k-armed bandit problem, so named by analogy to a slot machine.\n",
        "\n",
        "**Definitions:**\n",
        "\n",
        "1. Action-value function\n",
        "\\begin{align}\n",
        "q (a) = \\mathbb{E} [r_{t} | a_{t} = a]\n",
        "\\end{align}\n",
        "\n",
        "2. Action-value updates:\n",
        "\\begin{align}\n",
        "q_{t}(a) \\leftarrow q_{t}(a) + \\alpha (r_{t} - q_{t}(a))\n",
        "\\end{align}\n",
        "\n",
        "3. Greedy policy:\n",
        "\\begin{align}\n",
        "a_{t} = \\text{argmax}_{a} \\; q_{t} (a)\n",
        "\\end{align}\n",
        "\n",
        "4. $\\epsilon$-greedy policy:\n",
        "\\begin{align}\n",
        "P (a_{t} = a) = \n",
        "        \\begin{cases}\n",
        "        1 - \\epsilon    & \\quad \\text{if } a_{t} = \\text{argmax}_{a} \\; q_{t} (a) \\\\\n",
        "        \\epsilon/(N-1)        & \\quad \\text{else} \n",
        "        \\end{cases} \n",
        "\\end{align}\n"
      ]
    },
    {
      "metadata": {
        "id": "BKz4BzWSJulI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Please run the following code for your implementations:**"
      ]
    },
    {
      "metadata": {
        "id": "IE6OBiZmJlZK",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "% matplotlib inline\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from pylab import *"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nHferlEuANAS",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "class n_armed_bandit():\n",
        "    \"\"\"\n",
        "    World: N-Armed bandit.\n",
        "    Only one state, multiple actions.\n",
        "    Each action returns different amount of reward.\n",
        "    For each action, rewards are randomly sampled from normal distribution, \n",
        "        with a mean associated with that arm and unit variance.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, arm_number):\n",
        "        self.name = \"n_armed_bandit\"\n",
        "        self.n_states = 1\n",
        "        self.n_actions = arm_number\n",
        "        self.dim_x = 1\n",
        "        self.dim_y = 1\n",
        "        \n",
        "        self.mu = [np.random.normal(0,1) for a in range(self.n_actions)]\n",
        "        \n",
        "    def get_outcome(self, state, action):\n",
        "        \n",
        "        self.rewards = [np.random.normal(self.mu[i],1) for i in range(self.n_actions)]\n",
        "        next_state = None\n",
        "        \n",
        "        reward = self.rewards[action]\n",
        "        return int(next_state) if next_state is not None else None, reward"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5bzTxxIAmFJY",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "class drifting_n_armed_bandit():\n",
        "    \"\"\"\n",
        "    World: N-Armed bandit.\n",
        "    Only one state, multiple actions.\n",
        "    Each action returns different amounts of rewards.\n",
        "    For each action, rewards are randomly sampled from normal distribution, \n",
        "        with a mean associated with that arm and unit variance.\n",
        "    In the case of the non-stationary bandit, the mean reward associated with each arm \n",
        "        follows a Gaussian random.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, arm_number, drift):\n",
        "        self.name = \"drifting_n_armed_bandit\"\n",
        "        self.n_states = 1\n",
        "        self.n_actions = arm_number\n",
        "        self.dim_x = 1\n",
        "        self.dim_y = 1\n",
        "        \n",
        "        self.mu_min = 0.25\n",
        "        self.mu_max = 0.75\n",
        "        self.drift = drift\n",
        "        \n",
        "        self.mu = [np.random.normal(0,0) for a in range(self.n_actions)]\n",
        "\n",
        "    def update_mu(self):\n",
        "        self.mu += np.random.normal(0, self.drift, self.n_actions)\n",
        "            \n",
        "    def get_outcome(self, state, action):\n",
        "        \n",
        "        self.update_mu()\n",
        "        self.rewards = [np.random.normal(self.mu[i],1) for i in range(self.n_actions)]\n",
        "        next_state = None\n",
        "        \n",
        "        reward = self.rewards[action]\n",
        "        return int(next_state) if next_state is not None else None, reward"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Wn-Pq-99AP-E",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "def e_greedy_policy(q, epsilon):\n",
        "    if np.random.uniform(0,1) > epsilon:\n",
        "        return np.argmax(q)\n",
        "    else:\n",
        "        # choose randomly from all but the highest q value\n",
        "        a_list = np.arange(len(q))\n",
        "        arg_max = np.argmax(q)\n",
        "        choices = np.delete(a_list, arg_max)\n",
        "        return np.random.choice(np.delete(a_list, arg_max))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "V42V5kjv1qBY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# **Exercise 1:**\n",
        "\n",
        "Estimate the action value function of each of the $k$ actions in the bandit problem using the above update rule while choosing actions using an $\\epsilon$-greedy policy to choose actions. Run for 1000 steps (i.e. take 1000 actions for a set of bandits that each have a fixed probability of returning a reward). \n",
        "\n",
        "Plot reward obtained on each timestep for a randomly chosen bandit problem (i.e. a set of bandit arms with randomly chosen probabilities of providing rewards). \n"
      ]
    },
    {
      "metadata": {
        "id": "UwBFqGdm6FJX",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VOQoB1fjiMke",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# **Exercise 2:**\n",
        "\n",
        "1. Repeat above for 1000 runs (i.e. reset the bandit environment 1000 times, each time the bandit arms will get associated with randomly chosen probabilities of returning a reward), for different values of $\\epsilon \\in \\{0,0.1,0.01\\}$. Use $\\alpha = 0.01$.\n",
        "2. Plot performance curves i.e. fraction of runs on which optimal action was chosen at each time step, for the different values of $\\epsilon$. At each step check over all 1000 runs to determine the fraction of runs in which the optimal action was chosen. \n",
        "3. Change the learning rate $\\alpha$ and plot performance curves. \n",
        "4. Plot average rewards and performance curves using decaying learning rates of $\\alpha = 1/N(a)$, where $N(a)$ is the number of times bandit arm $a$ was chosen. \n",
        "5. Can you describe how $\\epsilon$ and $\\alpha$ affect the agent's performance? Tip: pay attention to the y-axis in your plots.\n",
        "6. (Optional) For which value of $\\epsilon$ will the above perform best in the long run in terms of cumulative reward and probability of selecting the best action? How much better will it be? Express your answer quantitatively."
      ]
    },
    {
      "metadata": {
        "id": "eyAVv4ROiSVr",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iN_H4boX6FY_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# **Exercise 3:**\n",
        "\n",
        "Estimate action values using an $\\epsilon$-greedy policy, using *optimistic starts* i.e. initialise your action value function to be a value greater than zero ($q_{0}(a)> 0$). \n",
        "\n",
        "Use $q_{0}(a) = 5$ and a fixed learning rate $\\alpha = 0.1$. \n",
        "\n",
        "1. Plot performance curves of $\\epsilon$-greedy action selection (for $\\epsilon = 0.1$) and using optimistic starts with greedy action selection. Overlay these plots.\n",
        "2. How does performance change for different values of $\\epsilon$?\n",
        "3. Change $q_{0}(a)$ and plot performance curves for different optimistic initial values. \n",
        "4. Can you explain why the initialisation is causing a difference in performance?\n"
      ]
    },
    {
      "metadata": {
        "id": "oazysvgj_TRU",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dkiyyuWg_Tqy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# **Exercise 4:**\n",
        "\n",
        "Estimate action values using an $\\epsilon$-greedy policy in the context of a nonstationary bandit problem. Use $\\epsilon = 0.1$.\n",
        "\n",
        "In this case, the expected rewards from each of the bandit arms drifts slowly and independently, based on a Gaussian random walk (with mean 0 and SD 0.025 and reflecting boundaries at 0.25 and 0.75). Number of bandit arms $k = 4$. \n",
        "\n",
        "1. What differences do you expect when using a fixed vs decaying learning rate in this context? \n",
        "2. For a single run, plot the expected reward from each bandit arm and the reward obtained by the agent. Run for 10000 steps. Try with fixed $\\alpha = 0.1$ and decaying learning rates $\\alpha = 1/N(a)$. \n",
        "3. Simulate 2000 runs and plot performance curves i.e. the fraction of runs on which the optimal arm was chosen at each step.\n",
        "4. How does the performance change for different learning rates? Plot performance curves using different values of $\\alpha = \\{0.001, 0.01, 0.1, 0.2\\}$ on the same plot.\n"
      ]
    },
    {
      "metadata": {
        "id": "igroydr6ktw6",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BOcbtRgZD0zb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# **(Optional) Exercise 5:**\n",
        "\n",
        "In the case of a decaying learning rate  of $\\alpha = 1/N(a)$, at the $n$th timestep of encountering the $a$th bandit arm, each past instance of choosing that arm is equally weighted.\n",
        "\n",
        "Can you analytically show that the above statement is true?\n",
        "\n",
        "When $\\alpha \\neq 1/N(a)$, the resulting estimate is a weighted sum of all previously encountered rewards from that arm.\n",
        "1. Compute analytically or plot the weighting of all previous rewards at the $n$th step in the case of constant learning rate of $\\alpha = 0.01$. \n",
        "2. What would be the weighting of all previous rewards from a given bandit arm at the $n$th step for an arbitrary sequence of learning rates $\\{ \\alpha_{1}, ... \\alpha_{n}\\}$? "
      ]
    },
    {
      "metadata": {
        "id": "kDKZd7yhb1H8",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}