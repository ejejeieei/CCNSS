{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"3_tutorial3_Sequential_decision_making_Two_step_task.ipynb","version":"0.3.2","views":{},"default_view":{},"provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"cells":[{"metadata":{"id":"IilJKOpR-qvR","colab_type":"text"},"cell_type":"markdown","source":["# **Introduction**\n","\n","In this tutorial we will implement an agent in a sequential decision making task using model-free and model-based approaches. To begin we will introduce the notion of contextual-bandits, which provide an intermediate step between the bandit problem we discussed in the previous tutorial and the full reinforcement learning problem."]},{"metadata":{"id":"z__QsqK6Rrnj","colab_type":"text"},"cell_type":"markdown","source":["# **Part I: Introduction**\n","\n","**Contextual bandits:**\n","\n","- Suppose there are several different k-armed bandit tasks, and that on each step you confront one of these chosen at random. Thus, the bandit task changes randomly from step to step. This would appear to you as a single, nonstationary k-armed bandit task whose true action values change randomly from step to step.\n","- Now suppose, that when a bandit task is selected, you are given some distinctive clue about its identity (but not its action values). Maybe you are facing an actual slot machine that changes the color of its display as it changes its action values. Now you can learn a policy associating each context with the best action to take when facing that context.\n","- This is an example of an associative search task and is often called contextual bandits.\n","- Learning value functions in these task requires the agent to maintain a representation of both the contexts (which we will refer to as a state) and the actions associated with them. \n","- State-action value functions for contextual bandits are defined and updated in the same way as those in the bandit task.\n","\n","**Definitions:**\n","\n","State-action value function:\n","\n","\\begin{align}\n","q (s, a) = \\mathbb{E} [r_{t} | s_{t} = s,a_{t} = a]\n","\\end{align}\n","\n","State-action value updates:\n","\n","\\begin{align}\n","q(s_{t}, a_{t}) \\leftarrow q(s_{t}, a_{t}) + \\alpha (r_{t} - q(s_{t}, a_{t}))\n","\\end{align}"]},{"metadata":{"id":"0nR_14eDfiD8","colab_type":"text"},"cell_type":"markdown","source":["**Please run the following code for your implementation:**"]},{"metadata":{"id":"K-9tkCHyfxDr","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["% matplotlib inline\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from pylab import *\n","import copy"],"execution_count":0,"outputs":[]},{"metadata":{"id":"qLRNJWzLo_N3","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["class drifting_probabilitic_bandit():\n","    \"\"\"\n","    World: 2-Armed bandit.\n","    Each arm returns reward with a different probability.\n","    The probability of returning rewards for all arms follow Gaussian random walks.\n","    \"\"\"\n","    \n","    def __init__(self, arm_number, drift):\n","        self.name = \"n_armed_bandit\"\n","        self.n_states = 1\n","        self.n_actions = arm_number\n","        self.dim_x = 1\n","        self.dim_y = 1\n","        \n","        self.mu_min = 0.25\n","        self.mu_max = 0.75\n","        self.drift = drift\n","        \n","        self.reward_mag = 1\n","        \n","        self.mu = [np.random.uniform(self.mu_min, self.mu_max) for a in range(self.n_actions)]\n","        \n","    def update_mu(self):\n","        self.mu += np.random.normal(0, self.drift, self.n_actions)\n","        self.mu[self.mu > self.mu_max] = self.mu_max\n","        self.mu[self.mu < self.mu_min] = self.mu_min\n","            \n","    def get_outcome(self, state, action):\n","        \n","        self.update_mu()\n","        self.rewards = [self.reward_mag if np.random.uniform(0,1) < self.mu[a] else 0 for a in range(self.n_actions) ]\n","        next_state = None\n","        \n","        reward = self.rewards[action]\n","        return int(next_state) if next_state is not None else None, reward"],"execution_count":0,"outputs":[]},{"metadata":{"id":"00gRPxvao_N6","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["class contextual_bandits(drifting_probabilitic_bandit):\n","  \n","    def __init__(self):\n","        \n","        self.name = \"contextual_bandit\"\n","        self.n_states = 3\n","        self.n_actions = 2\n","        self.dim_x = 1\n","        self.dim_y = 1\n","        \n","        self.n_arms = self.n_actions\n","        self.n_of_bandits = self.n_states\n","        self.drift = 0.02\n","        self.bandits = [drifting_probabilitic_bandit(self.n_arms, self.drift) for n in range(self.n_of_bandits)]\n","\n","    def get_outcome(self, state, action):\n","        \n","        _, reward = self.bandits[state].get_outcome(0,action)\n","        available_states = [s for s in range(self.n_of_bandits) if s != state]\n","        next_state = np.random.choice(available_states)\n","        \n","        return int(next_state) if next_state is not None else None, reward"],"execution_count":0,"outputs":[]},{"metadata":{"id":"UP-QsjDto_N_","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["def e_greedy_policy(q, epsilon):\n","    if np.random.uniform(0,1) > epsilon:\n","        return np.argmax(q)\n","    else:\n","        # choose randomly from all but the highest q value action\n","        a_list = np.arange(len(q))\n","        arg_max = np.argmax(q)\n","        choices = np.delete(a_list, arg_max)\n","        return np.random.choice(np.delete(a_list, arg_max))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"zH6nrqLVBPwW","colab_type":"text"},"cell_type":"markdown","source":["# **Exercise 1:**\n","\n","Train an agent to estimate the state-action value function for the contextual bandit task. The task has three contexts or states, each allows a two-arm bandit choice. On each step, the environment will provide a context and the agent is required to select an action. Select actions using an $\\epsilon$-greedy policy. \n","\n","Plot performance curves for each of the bandits in each of the contexts."]},{"metadata":{"id":"KLMYpPd3F5Ct","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"Wiw6H5PkJCM-","colab_type":"text"},"cell_type":"markdown","source":["# **Part II: Introduction**"]},{"metadata":{"id":"8l5_IOTbd_2p","colab_type":"text"},"cell_type":"markdown","source":["We will now extend the contextual bandit task to a sequential decision making task which provides a simple, yet very interesting environment that captures many aspects of the full reinforcement learning problem. The state-action value function in this case not only accounts for immediate rewards, but also future rewards. Also, actions in this environment allow the agent to influence the next state it encounters.\n","\n","We will train an agent to solve this task using, both, model-free and model based strategies. For the model-free case, at each step the agent updates its estimate of its state-action value function using TD-learning. \n","\n","For the model-based case, on each step, we update a state transition matrix using a state-prediction error and an estimate of expected reward from each end state using the same update rule for the SARSA implementation. Finally, we explicitly compute the model-based state-action value function $Q_{MB}$ using the state transition matrix and expected rewards from the end states.\n","\n","\n","\n","**Daw two-step task description:**\n","- The task is a two-stage Markov decision task in which, on each trial, an initial choice between two options probabilistically leads to either of two, second-stage ‘‘states’’.\n","- In turn, these second stage states demand another two-option choice, each of which is associated with a different chance of delivering a reward.\n","- The choice of one first-stage option leads predominantly (70% of the time) to one of the two second-stage states, and this relationship is fixed.\n","- The chances of payoff associated with the four second-stage options, however, change slowly and independently, according to Gaussian random walks.\n","\n","**Definitions:**\n","\n","For the general case, we define a state-action value function as:\n","\n","\\begin{align}\n","Q (s, a) = \\mathbb{E} [r_{t} + \\gamma r_{t+1} + \\gamma^2 r_{t+2} + ... | s_{t} = s,a_{t} = a]\n","\\end{align}\n","\n","\\begin{align}\n","= \\mathbb{E} [r_{t} + \\gamma Q(s', a') | s_{t} = s,a_{t} = a]\n","\\end{align}\n","\n","State-action value updates (SARSA): \n","\n","\\begin{align}\n","Q(s_{t}, a_{t}) \\leftarrow Q(s_{t}, a_{t}) + \\alpha (r_{t} + \\gamma Q(s_{t+1},a_{t+1}) - Q(s_{t}, a_{t}))\n","\\end{align}\n","\n","Soft-max policy:\n","\n","\\begin{align}\n","P(a_{t}|s_{t}) = \\frac{ e^{\\beta Q(s_{t}, a_{t})} }{ \\sum_{b_{t}} e^{\\beta Q(s_{t}, b_{t})} }\n","\\end{align}\n","\n","State transition matrix:\n","\n","\\begin{align}\n","T(s,a,s') = P(s_{t+1} = s' | s_{t} = s, a_{t} = a)\n","\\end{align}\n","\n","Model-based state-action value function:\n","\n","\\begin{align}\n","Q_{MB}(s,a) =  r_{t} + \\sum_{s'} T(s,a,s') \\: \\text{max}_{a'} Q(s',a')\n","\\end{align}"]},{"metadata":{"id":"X_Mlg_ano_OL","colab_type":"text"},"cell_type":"markdown","source":["**Please run the following code for your implementation:**"]},{"metadata":{"id":"nrsyW3h6Pspr","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["class Daw_two_step_task(drifting_probabilitic_bandit):\n","  \n","    def __init__(self):\n","        \n","        self.name = \"Daw_two_step_task\"\n","        self.n_states = 3\n","        self.n_actions = 2\n","        self.dim_x = 1\n","        self.dim_y = 1\n","        \n","        self.n_arms = self.n_actions\n","        self.n_of_bandits = 2\n","        self.drift = 0.02\n","        \n","        self.context_transition_prob = 0.7\n","        self.bandits = [drifting_probabilitic_bandit(self.n_arms, self.drift) for n in range(self.n_of_bandits)]\n","        \n","    def get_outcome(self, state, action):\n","        \n","        if state == 0:\n","            reward = 0\n","            if action == 0:\n","                if np.random.uniform(0,1) < self.context_transition_prob:\n","                    next_state = 1\n","                else:\n","                    next_state = 2\n","            elif action == 1:\n","                if np.random.uniform(0,1) < self.context_transition_prob:\n","                    next_state = 2\n","                else:\n","                    next_state = 1\n","            else:\n","                print('No valid action specified')\n","                \n","        if state == 1:\n","            _, reward = self.bandits[0].get_outcome(0, action)\n","            next_state = 0\n","            \n","        if state == 2:\n","            _, reward = self.bandits[1].get_outcome(0, action)\n","            next_state = 0\n","        \n","        return int(next_state) if next_state is not None else None, reward"],"execution_count":0,"outputs":[]},{"metadata":{"id":"-6GnM2MHo_ON","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["def softmax(state, q, beta):\n","    \"\"\"\n","    Softmax policy: selects action probabilistically depending on the value.\n","    Args:\n","        state: an integer corresponding to the current state.\n","        q: a matrix indexed by state and action.\n","        params: a dictionary containing the default parameters.\n","    Returns:\n","        an integer corresponding to the action chosen according to the policy.\n","    \"\"\"\n","    \n","    value = q[state,:]\n","    prob = exp(value * beta) # beta is the inverse temperature parameter\n","    prob = prob / sum(prob)  # normalize\n","    cum_prob = cumsum(prob)  # cummulation summation\n","    action = where(cum_prob > rand())[0][0]\n","    return action"],"execution_count":0,"outputs":[]},{"metadata":{"id":"7ycVpA3PYazZ","colab_type":"text"},"cell_type":"markdown","source":["# **Exercise 2:**\n","\n","Implement the two-step task using a model-free strategy. To do so, train an agent on the above two-step task using SARSA updates. Run 3000 trials. Use a constant learning rate $\\alpha = 0.2$ and a soft-max policy using an inverse temperature parameter of $\\beta = 4$. \n","\n","1. For a single run, plot the reward probability associated with each of the second stage choices on each trial, the q-values learnt by the agent for all second state actions and the q-values of the two first stage actions.\n","2. Vary parameters $\\alpha$ and $\\beta$. How do these parameters influence the agent's estimates?\n","3. Each trial can be characterised by whether or not reward was received and whether the second-stage presented was common or rare given the first-stage choice. For each of these trial types, plot the probability of repeating the first-stage choice on the next trial.\n","4. How do the parameters $\\alpha$ and $\\beta$ affect the probabilities in (3)?"]},{"metadata":{"id":"md5sEEs_YSVX","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"seIh4U3vYSw7","colab_type":"text"},"cell_type":"markdown","source":["# **Exercise 3:**\n","\n","We will now implement the two-step task using a model-based strategy. To do so, estimate a state transision matrix $T(s,a,s')$ and expected reward from each end state-action pair $Q(s,a)$. Initialise the transition matrix to have uniform probabilities of transitioning to all possible next states. Run 3000 trials. Use constant learning rates $\\alpha = 0.2$ and a soft-max policy using an inverse temperature parameter of $\\beta = 4$. \n","\n","1. Plot the expected reward from each of the second stage choices on each trial and the actual reward obtained by the agent.\n","2. Vary parameters $\\alpha$ and $\\beta$. How do these parameters influence the agent's performance?\n","3. For each of the four trial types discussed in the previous exercise, plot the probability of repeating the first-stage choice on the next trial when using a model-based strategy. \n","4. How do the parameters $\\alpha$ and $\\beta$ affect the probabilities in (3)? \n","5. How does the agent's performance using the model-based strategy compare with that using a model-free strategy? How do the stay probabilities differ when using a model-based vs model-free strategy?\n","\n"]},{"metadata":{"id":"3GQLD4ItkDXd","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"t9VXkj19kD1m","colab_type":"text"},"cell_type":"markdown","source":["# **Exercise 4:**\n","\n","We will now implement an agent that is a hybrid version of the model-free and model-based agents. Simultaneously train a model based and model free agent (as above). Run 5000 trials. Choose actions using a soft-max policy that uses a weighted combination of the model-based and model-free Q-values. Test for $w_{1} \\in [0.3, 0.6]$ and $w_{2} = 1- w_{1}$.\n","\n","1. Plot the expected reward from each of the second stage choices on each trial and the actual reward obtained by the agent.\n","2. For each of the four trial types discussed in the previous exercise, plot the probability of repeating the first-stage choice on the next trial when using the hybrid strategy.\n","3. Often, subjects are tested on this task and their stay probabilities are fit using such a hybrid model. The relative magnitude of the weights $w(1)$ and $w(2)$ is often taken to indicate how much they rely on model-based vs model-free strategies for behavioural control. Under what assumptions does this task allow us to do so? Hint: what components in your model simulation were required for you to be able to reproduce the profiles of stay-probabilities associated with model-based and model-free control?"]},{"metadata":{"id":"Uyrbvxflo_OV","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]}]}