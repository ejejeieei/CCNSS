{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bIfRGL4ZTO_j"
   },
   "source": [
    "## CCNSS 2018 Module 3: Reinforcement learning and planning\n",
    "# Tutorial 1: Value estimation during classical conditioning using TD-learning\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "v_f1mBU0VnsO"
   },
   "source": [
    "# **Introduction **\n",
    "\n",
    "In this tutorial, we will learn how to estimate state-value functions in a classical conditioning paradigm using Temporal Difference (TD) learning and examine TD-errors at the presentation of the conditioned and unconditioned stimulus (CS and US) under different CS-US contingencies.\n",
    "\n",
    "__Environment:__\n",
    "\n",
    "- The classical conditioning environment is composed of a sequence of states that the agent deterministically transitions through. \n",
    "- The agent experiences the environment in episodes or trials. \n",
    "- Episodes terminate by transitioning to the inter-trial-interval (ITI) state and they are initiated from the ITI state as well. We clamp the value of the terminal/ITI states to zero. \n",
    "- Within each episode, the agent is presented a CS and US (reward). \n",
    "- For each exercise, we use a different CS-US contingency. \n",
    "- The goal of the agent is to learn to predict expected rewards from each state in the trial. \n",
    "\n",
    "\n",
    "__Definitions:__\n",
    "\n",
    "1. Returns: \n",
    "\\begin{align}\n",
    "G_{t} = r_{t+1} + \\gamma r_{t+2} + \\gamma^2 r_{t+3} + ... = \\sum \\limits_{k = 1}^{\\infty} \\gamma^{k-1} r_{t+k}\n",
    "\\end{align}\n",
    "\n",
    "2. Value: \n",
    "\\begin{align}\n",
    "V(s_{t}) = \\mathbb{E} [ G_{t} | s_{t}] = \\mathbb{E} [r_{t+1} + \\gamma V_{t+1} | s_{t}] \n",
    "\\end{align}\n",
    "\n",
    "3. TD-error:\n",
    "\\begin{align}\n",
    "\\delta_{t} = r_{t+1} + \\gamma V(s_{t+1}) - V(s_{t})\n",
    "\\end{align}\n",
    "\n",
    "4. Value updates:\n",
    "\\begin{align}\n",
    "V(s_{t}) \\leftarrow V(s_{t}) + \\alpha \\delta_{t}\n",
    "\\end{align}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KCHJCQ6akx54"
   },
   "source": [
    "**Run the following code for your implementation:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "I5qMzMZHkuLj"
   },
   "outputs": [],
   "source": [
    "% matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pylab import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "wAdvEkp-VHXO"
   },
   "outputs": [],
   "source": [
    "class classical_conditioning():\n",
    "    \n",
    "    def __init__(self, n_steps):\n",
    "        \n",
    "        # Task variables\n",
    "        self.n_steps = n_steps \n",
    "        self.n_actions = 0\n",
    "        \n",
    "        # Reward variables\n",
    "        self.reward_state = [0,0]\n",
    "        self.reward_magnitude = reward_magnitude\n",
    "        self.reward_probability = reward_probability\n",
    "        self.reward_time = reward_time\n",
    "        \n",
    "        # Time step at which the conditioned stimulus is presented\n",
    "        self.cs_time = int(n_steps/4) - 1\n",
    "\n",
    "        # Create a state dictionary\n",
    "        self.create_state_dictionary()\n",
    "    \n",
    "    def define_reward(self, reward_magnitude, reward_time):\n",
    "        \n",
    "        \"\"\"\n",
    "        Determine reward state and magnitude of reward\n",
    "        \"\"\"\n",
    "        if reward_time >= self.n_steps - self.cs_time:\n",
    "            self.reward_magnitude = 0\n",
    "        \n",
    "        else:\n",
    "            self.reward_magnitude = reward_magnitude\n",
    "            self.reward_state = [1, reward_time]\n",
    "            \n",
    "    def get_outcome(self, current_state, action = 0):\n",
    "    \n",
    "        \"\"\"\n",
    "        Determine next state and reward\n",
    "        \"\"\"\n",
    "        # Update state\n",
    "        if current_state < self.n_steps - 1: \n",
    "            next_state = current_state + 1\n",
    "        else:\n",
    "            next_state = 0\n",
    "            \n",
    "        # Check for reward\n",
    "        if self.reward_state == self.state_dict[current_state]:\n",
    "            reward = self.reward_magnitude\n",
    "        else:\n",
    "            reward = 0\n",
    "        \n",
    "        return next_state, reward\n",
    "    \n",
    "    def create_state_dictionary(self):\n",
    "        \n",
    "        \"\"\"\n",
    "        This dictionary maps number of time steps/ state identities\n",
    "        in each episode to some useful state attributes:\n",
    "        \n",
    "        state      - 0 1 2 3 4 5 (cs) 6 7 8 9 10 11 12 ...\n",
    "        is_delay   - 0 0 0 0 0 0 (cs) 1 1 1 1  1  1  1 ...\n",
    "        t_in_delay - 0 0 0 0 0 0 (cs) 1 2 3 4  5  6  7 ...\n",
    "        \"\"\"\n",
    "        d = 0\n",
    "\n",
    "        self.state_dict = {}\n",
    "        for s in range(self.n_steps):\n",
    "            if s <= self.cs_time:\n",
    "                self.state_dict[s] = [0,0]\n",
    "            else: \n",
    "                d += 1 # Time in delay           \n",
    "                self.state_dict[s] = [1,d]\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unless specified otherwise, use the following parameter values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_time = 10\n",
    "reward_magnitude = 10\n",
    "reward_probability = 1\n",
    "\n",
    "n_trials = 20000\n",
    "n_steps = 40\n",
    "\n",
    "gamma = 0.98  # temporal discount factor\n",
    "alpha = 0.001 # learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ei_bAO1dY1fu"
   },
   "source": [
    "# **Exercise 1:**\n",
    "\n",
    "Implement TD-learning to estimate the state-value function in the classical-conditioning world with guaranteed rewards, with a fixed magnitude, at a fixed delays after the CS. Save the history of estimated value function and TD-errors over learning.\n",
    "\n",
    "1. Plot TD-errors on an example trial before learning.\n",
    "2. Plot the estimated value function for each state after learning has converged.\n",
    "3. Plot the value function and TD-errors over learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "QPW--3PDgfix"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TIj3B4xKCph0"
   },
   "source": [
    "# **Exercise 2:**\n",
    "\n",
    "Implement TD-learning in the context of rewards of varying magnitudes. Use a set of discrete reward magnitudes. On each trial, randomly select a reward from the specified set. \n",
    "1. Plot TD-errors on a set of trials before learning, as many as the number of reward magnitudes. On each trial present a different reward magnitude. Overlay the TD-errors on these set of trials on the same plot.\n",
    "2. Plot the estimated state-value function for each state after learning has converged.\n",
    "3. Plot TD-errors on a set of trials, on for each reward magnitude (as in (1)), after learning has converged.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_magnitudes = [10, 6, 4, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "K4_LiUpMYF7G"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fYQ3kNXkiGxH"
   },
   "source": [
    "# **Exercise 3: **\n",
    "\n",
    "Implement TD-learning in the context of probabilistic rewards. For a fixed probability of reward delivery ($P(r) < 1$), on each trial randomly determine whether or not to present a reward on that trial. Keep reward magnitude constant.\n",
    "1. Plot the estimated value function after learning.\n",
    "2. Plot TD-errors on a trial with reward delivered and one with reward omitted, after learning. Overlay the two on the same plot.\n",
    "3. Repeat (1) and (2) for different probabilities of reward delivery. How does the value function and TD-errors change for different probabilities of reward delivery?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_probabilities = [0.1, 0.4, 0.6, 0.9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "mOVzho6aiKc0"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tm2U3wu_YGS9"
   },
   "source": [
    "# **Exercise 4:**\n",
    "\n",
    "Implement TD-learning in the context of rewards presented at variable delays since CS. Use a set of discrete times since CS. On each trial, randomly select the delay between CS ans US. \n",
    "\n",
    "1. Plot TD-errors on a set of trials before learning, as many as the number of CS-US delays. On each trial present a different delay between the CS and reward (US). Overlay the TD-errors on these set of trials on the same plot. \n",
    "2. Plot the estimated state value function after learning. \n",
    "3. Plot TD-errors on a set of trials, one for each delay duration, after learning. \n",
    "4. Implement (1) to (3) in the context of probabilitic rewards ($P(r) < 1$). On each trial, randomly select the delay between CS-US and whether or not to present a reward on that trial. How do TD-errors change when the reward probability is changed?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start by using: reward_times = [10 20]\n",
    "# Once you think you understand why the estimated value function looks the way it does,\n",
    "# then try reward_times = np.arange(10,21,3) and reward_times = np.arange(10,21,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "8zntG8Ndl8EO"
   },
   "outputs": [],
   "source": [
    "reward_probabilities = [0.1, 0.4, 0.6, 0.9, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gHdASwF5iVe_"
   },
   "source": [
    "# **Exercise 5: **\n",
    "\n",
    "In this exercise we will implement a commonly used heuristic used in modeling activity of dopamine neurons, TD-reset. \n",
    "Implement TD-learning as in previous exercises, but set TD-error to zero on all steps after reward (US). \n",
    "\n",
    "1. Plot value function and TD-errors. \n",
    "2. Can you explain how the reset is changing the TD-errors and value function?\n",
    "3. How do they change if rewards are probabilistic ($P(r) < 1$)? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_times = np.arange(10,21,1)\n",
    "reward_probabilities = [0.1, 0.4, 0.6, 0.9, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "4-cmWEQ0iZYa"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pi3vvJBFl8Uv"
   },
   "source": [
    "# **Exercise 6:**\n",
    "\n",
    "Implement Exercise 5, after making the following small change: instead of resetting the TD-error, set the next_state after reward delivery as the ITI state, irrespective of the time of reward. \n",
    "\n",
    "1. Compare value functions and TD-errors in Exercise 5 with those using the modified environment. \n",
    "2. Can you explain the differences in these two cases?\n",
    "3. How does this change in the state representation compare with the reset we introduced in Exercise 4 in terms of our interpretation regarding how an agent understand the classical conditioning paradigm?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "SNdwqRENn3Ib"
   },
   "outputs": [],
   "source": [
    "reward_times = np.arange(10,21,1)\n",
    "reward_probabilities = [0.1, 0.4, 0.6, 0.9, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Are you done?\n",
    "\n",
    "Awesome! In that case, ask me and I will point you to an interesting paper to read. Given these results, you can evaluate the main claims in that paper with a more critical eye! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "2018_Module3_Tutorial1_Value_estimation_classical_condiitoning.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
