{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrence, Depth and High-dimensional data\n",
    "# Machine learning basics notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we present basic methods to train and evaluate the performance of machine learning models. In order to keeps things simple, we'll use a simple linear model, called *polynomial regression*, to fit data that was generated synthetically.\n",
    "\n",
    "The following elements will be presented:\n",
    "\n",
    "* polynomial regression\n",
    "* train and test sets\n",
    "* train and validation loss\n",
    "* cross-validation for model comparison\n",
    "* regularization by dataset size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**References:**\n",
    "* [Pattern recognition and machine learning](http://www.springer.com/gp/book/9780387310732), chapter 1, Bishop, Christopher M, springer, 2006."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Please execute the cell bellow in order to initialize the notebook environment*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autosave 0\n",
    "# %matplotlib inline\n",
    "%matplotlib notebook\n",
    "\n",
    "from __future__ import division\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import mod3\n",
    "\n",
    "plt.rcParams.update({'figure.figsize': (5.0, 4.0), 'lines.linewidth': 2.0})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polynomial regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose a process generated $n$ data points $\\mathcal{D}=\\{(x_1,y_1),\\ldots,(x_n,y_n)\\}$, where each $y_i$ is a function of $x_i$ and noise $\\xi_i$. We write $y_i=f(x_i, \\xi_i)$ for the unknown function $f()$ that models the response of the process, and the goal is to estimate $\\hat{y}$ for any value of $x$.\n",
    "\n",
    "Polynomial regression assumes the data generation process can be modeled as polynomial of $x$ with an additive noise term $\\xi$. For a polynomial of degree $p$ this is written as:\n",
    "$$\n",
    "y = w_0 + w_1 x + w_2 x^2 + w_3 x^3 + \\cdots + w_p x^p + \\xi \n",
    "$$\n",
    "\n",
    "For all data points in $\\mathcal{D}$, this can be written as a system of linear equations:\n",
    "\n",
    "$$\n",
    "{\\begin{bmatrix}y_{1}\\\\y_{2}\\\\y_{3}\\\\\\vdots \\\\y_{n}\\end{bmatrix}}={\\begin{bmatrix}1&x_{1}&x_{1}^{2}&\\dots &x_{1}^{p}\\\\1&x_{2}&x_{2}^{2}&\\dots &x_{2}^{p}\\\\1&x_{3}&x_{3}^{2}&\\dots &x_{3}^{p}\\\\\\vdots &\\vdots &\\vdots &&\\vdots \\\\1&x_{n}&x_{n}^{2}&\\dots &x_{n}^{p}\\end{bmatrix}}{\\begin{bmatrix}w_{0}\\\\w_{1}\\\\w_{2}\\\\\\vdots \\\\w_{p}\\end{bmatrix}}+{\\begin{bmatrix}\\xi _{1}\\\\\\xi _{2}\\\\\\xi _{3}\\\\\\vdots \\\\\\xi _{n}\\end{bmatrix}}\n",
    "$$\n",
    "\n",
    "This can be written in matrix notation as $\\mathbf{y}=\\mathbf{X}\\mathbf{w}+\\boldsymbol{\\xi}$, where $\\mathbf{y}$ and $\\boldsymbol{\\xi}$ are $n×1$ vectors, $\\mathbf{X}$ is an $n×p$ matrix, and $\\mathbf{w}$ is a $p×1$ vector of unknown parameters. The matrix $\\mathbf{X}$ is sometimes called the *design matrix*.\n",
    "\n",
    "The ordinary least squares (OLS) solution minimizes the sum of squared residuals \n",
    "\n",
    "$$C(\\mathcal{D},\\mathbf{w})=\\frac{1}{2n}\\sum _{i=1}^{n}(y_{i}-\\mathbf{x}_{i}^{T}\\mathbf{w})^{2}=(\\mathbf{y}-\\mathbf{X}\\mathbf{w})^{T}(\\mathbf{y}-\\mathbf{X}\\mathbf{w}),\n",
    "$$\n",
    "\n",
    "and is given by:\n",
    "\n",
    "$$\n",
    "\\widehat{\\mathbf{w}}_{OLS}=(\\mathbf{X}^{T}\\mathbf{X})^{-1}\\;\\mathbf{X}^{T}\\mathbf{y}\n",
    "$$\n",
    "\n",
    "In this notebook, we'll use polynomial regression models to investigate properties and methods that are relevant to more advanced machine learning models. A function implementing the OLS solution is provided as `mod3.ols(x, y, p=1)` from the module `mod3`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXERCISE 1**\n",
    "\n",
    "The data $\\mathcal{D}=\\{(x_1,y_1),\\ldots,(x_n,y_n)\\}$ will be generated with the following procedure:\n",
    "\n",
    "* $x$ is sampled uniformly in the interval $[a,b]$, i.e. $x\\sim U(a,b)$\n",
    "* $\\xi$ is sampled as zero mean gaussian noise, i.e. $\\xi\\sim\\mathcal{N}(0, \\sigma^2)$\n",
    "* the dependent variable $y_i$ is obtained as a sinusoidal function of $x_i$ with additive noise $y_i=f(x_i, \\xi_i)=\\sin(2\\pi x_i) + \\xi_i$\n",
    "\n",
    "Write a function that returns $n$ values of $x$ and $y$ given the dataset size $n$ and noise parameters $a$, $b$ and $\\sigma$, i.e.\n",
    "```\n",
    "x, y = generate_data(n, a, b, sigma)\n",
    "```\n",
    "\n",
    "The function $f(\\mathbf{x}, \\boldsymbol{\\xi})$ is implemented as follows:\n",
    "```\n",
    "def f(x, xi=0):\n",
    "    return np.sin(2*np.pi*x) + xi\n",
    "```\n",
    "**INSTRUCTIONS**\n",
    "* use parameters $n=50000$, $a=0$, $b=1$ and $sigma=0.1$\n",
    "* write the function `f(x, xi=0)` as shown above\n",
    "* write the function `generate_data(n, a, b, sigma)`\n",
    "* generate the dataset and plot histograms of Numpy arrays `x` and `y`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sinusoidal signal + noise\n",
    "\n",
    "def f(x, xi=0):\n",
    "    \n",
    "    return np.sin(2*np.pi*x) + xi\n",
    "\n",
    "def generate_data(n, a, b, sigma):\n",
    "    \n",
    "    x = a + (b-a)*np.random.rand(n)\n",
    "    xi = sigma*np.random.randn(n)\n",
    "    \n",
    "    y = f(x, xi=xi)\n",
    "    \n",
    "    return (x, y)\n",
    "\n",
    "n = 50000\n",
    "a = 0\n",
    "b = 1\n",
    "sigma = 0.1\n",
    "\n",
    "x, y = generate_data(n, a, b, sigma)\n",
    "\n",
    "gs = gridspec.GridSpec(1, 2)\n",
    "\n",
    "plt.figure(figsize=(9, 4))\n",
    "\n",
    "plt.subplot(gs[0])\n",
    "plt.hist(x.flatten(), bins=64)\n",
    "plt.title('Histogram of x')\n",
    "\n",
    "plt.subplot(gs[1])\n",
    "plt.hist(y.flatten(), bins=64)\n",
    "plt.title('Histogram of y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXPECTED OUTPUT**\n",
    "\n",
    "<img src=\"fig/poly_fit_histogram.png\" style=\"width:90%;height:90%;display:inline;margin:1px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXERCISE 2**\n",
    "\n",
    "Generate a dataset $\\mathcal{D}=\\{\\mathbf{x}, \\mathbf{y}\\}$, and visualise with the data generating function with zero noise term, i.e. $f(x, \\xi=0)$.\n",
    "\n",
    "**INSTRUCTIONS**\n",
    "* generate a dataset with $n=25$, $a=0$, $b=1$ and $\\sigma=0.3$\n",
    "* plot the dataset and underlying data generating function noise term equal to 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sinusoidal signal + noise\n",
    "\n",
    "# data params\n",
    "n = 25\n",
    "a, b = (0, 1)\n",
    "sigma = 0.3\n",
    "\n",
    "# generate data\n",
    "x, y = generate_data(n, a, b , sigma)\n",
    "\n",
    "# plot data\n",
    "x_range = np.linspace(a-0.1*(b-a), b+0.1*(b-a), num=500)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(x_range, f(x_range), 'C2')\n",
    "mod3.plot_data(x, y)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXPECTED OUTPUT**\n",
    "\n",
    "<img src=\"fig/poly_fit_sample.png\" style=\"width:50%;height:50%;display:inline;margin:1px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polynomial regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXERCISE 3**\n",
    "\n",
    "Write a function `g(x, w)` that implements the polynomial model \n",
    "$$\\mathbf{y}=\\mathbf{X}\\mathbf{w}$$\n",
    "for design matrix $\\mathbf{X}$ and weights vector $\\mathbf{w}$.\n",
    "\n",
    "Write a function `loss(x, y, w)` that implements the loss function \n",
    "$$C(\\mathcal{D},\\mathbf{w})=\\frac{1}{2n}\\sum _{i=1}^{n}(\\mathbf{x}_{i}^{T}\\mathbf{w}-y_{i})^{2}$$\n",
    "where $\\mathbf{x}_{i}^{T}$ is a row vector of $\\mathbf{X}$, i.e. a vector containing the $p$ powers of $x_i$.\n",
    "\n",
    "**INSTRUCTIONS**\n",
    "* write the function g(x, w)\n",
    "* write the function loss(x, y, w)\n",
    "* validate by executing the code below:\n",
    "\n",
    "```\n",
    "x = np.array([1.5, 5.3, 3.9, 9.4, 5.5])\n",
    "y = np.array([4.4, 2.8, 6.1, 7.5, 2.3])\n",
    "w = np.array([[0.7],\n",
    "              [0.8],\n",
    "              [0.9]])\n",
    "\n",
    "print g(x, w)\n",
    "print loss(x, y, w)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def g(x, w):\n",
    "    \n",
    "    X = np.zeros((len(x), len(w)))\n",
    "    for i in range(len(w)): \n",
    "        X[:,i] = x**i\n",
    "        \n",
    "    return np.sum(np.dot(X, w), axis=1)\n",
    "\n",
    "def loss(x, y, w):\n",
    "    \n",
    "    return 0.5/len(x)*np.sum((g(x, w)-y)**2)\n",
    "\n",
    "x = np.array([1.5, 5.3, 3.9, 9.4, 5.5])\n",
    "y = np.array([4.4, 2.8, 6.1, 7.5, 2.3])\n",
    "w = np.array([[0.7],\n",
    "              [0.8],\n",
    "              [0.9]])\n",
    "\n",
    "print g(x, w)\n",
    "print loss(x, y, w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXPECTED OUTPUT**\n",
    "```\n",
    "[  3.925  30.221  17.509  87.744  32.325]\n",
    "822.2902308\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXERCISE 4**\n",
    "Fit a polynomial to the generated data, and compare it with the data generating function.\n",
    "\n",
    "**INSTRUCTIONS**\n",
    "* generate a dataset with $n=25$, $a=0$, $b=1$ and $\\sigma=0.3$\n",
    "* fit a polynomial of degree $p=4$ to the training data using the function `mod3.ols(x, y, p=deg)`\n",
    "* plot the generated data, the polynomial and the weights\n",
    "* you may use the function `mod3.plot_bars` to plot bars representing the weight values\n",
    "\n",
    "```\n",
    "mod3.plot_bars(w_ols, title='Weights', ax_labels=('idx', 'w'))```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# polynomial regression\n",
    "\n",
    "reload(mod3)\n",
    "\n",
    "# data params\n",
    "n = 25\n",
    "a, b = (0, 1)\n",
    "sigma = 0.3\n",
    "\n",
    "# generate data\n",
    "x, y = generate_data(n, a, b , sigma)\n",
    "\n",
    "deg = 4\n",
    "\n",
    "w_ols = mod3.ols(x, y, p=deg)\n",
    "\n",
    "x_range_ext = np.linspace(a-0.5*(b-a), b+0.5*(b-a), num=500)\n",
    "\n",
    "gs = gridspec.GridSpec(1, 3)\n",
    "\n",
    "plt.figure(figsize=(9, 3))\n",
    "\n",
    "plt.subplot(gs[0])\n",
    "plt.plot(x_range, g(x_range, w_ols), alpha=0.8, label='poly'+str())\n",
    "mod3.plot_data(x, y, title='Polynomial fit')\n",
    "\n",
    "plt.subplot(gs[1])\n",
    "plt.plot(x_range_ext, f(x_range_ext), 'C2', alpha=0.8)\n",
    "plt.plot(x_range_ext, g(x_range_ext, w_ols), alpha=0.8, label='poly'+str())\n",
    "mod3.plot_data(x, y, title='Polynomial fit')\n",
    "plt.xlim([x_range_ext[0], x_range_ext[-1]])\n",
    "\n",
    "plt.subplot(gs[2])\n",
    "mod3.plot_bars(w_ols, title='Weights', ax_labels=('idx', 'w'))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXPECTED OUTPUT**\n",
    "\n",
    "<img src=\"fig/poly_fit_weigths.png\" style=\"display:inline;margin:1px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXERCISE 5**\n",
    "\n",
    "A polynomial of degree $p$ can perfectly fit a dataset with $p+1$ elements.\n",
    "\n",
    "**INSTRUCTIONS**\n",
    "* generate a train set of size $n=4$, $a=0$, $b=1$ and $\\sigma=0.1$\n",
    "* train and test polynomials of degree up to $p=6$\n",
    "* plot the train loss for each polynomial degree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# expressive power of polynomials\n",
    "\n",
    "# data params\n",
    "n = 4\n",
    "a, b  = (0, 1)\n",
    "sigma = 0.1                                   \n",
    "deg_range = range(6)\n",
    "\n",
    "# generate data\n",
    "x_train, y_train = generate_data(n, a, b , sigma)\n",
    "\n",
    "# multiple polynomial fits\n",
    "\n",
    "gs = gridspec.GridSpec(2, 3)\n",
    "\n",
    "plt.figure(figsize=(9, 6))\n",
    "\n",
    "for idx, deg in enumerate(deg_range):\n",
    "\n",
    "    w_ols = mod3.ols(x_train, y_train, p=deg)\n",
    "    \n",
    "    loss_train = np.sqrt(2*loss(x_train, y_train, w_ols))\n",
    "    \n",
    "    plt.subplot(gs[idx])\n",
    "    plt.plot(x_range, g(x_range, w_ols))\n",
    "    mod3.plot_data(x_train, y_train, title='Polynomial fit')\n",
    "    plt.title('Loss deg '+str(deg)+': '+format(loss_train, '.3f'))\n",
    "    plt.xlim([x_range[0], x_range[-1]])\n",
    "    plt.ylim([-2, 2])\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXPECTED OUTPUT**\n",
    "\n",
    "<img src=\"fig/poly_fit_deg_many.png\" style=\"width:100%;height:100%;display:inline;margin:1px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXERCISE 6**\n",
    "\n",
    "A polynomial of degree $p$ can perfectly fit a dataset with $p+1$ elements, but in this case it is likely to be fitting the data, rather than the process generating the data, i.e. it is *overfitting*. Model overfitting can be detected by good performance on the train set and much worse performance on the test set.\n",
    "\n",
    "**INSTRUCTIONS**\n",
    "* generate a train set of size $n=10$, $a=0$, $b=1$ and $\\sigma=0.1$\n",
    "* generate a test set of size $n=2000$\n",
    "* train and test polynomials of degree up to $p=10$\n",
    "* plot the train and test loss for each polynomial degree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# train and test sets\n",
    "\n",
    "# data params\n",
    "n = 10\n",
    "n_test = 2000\n",
    "a, b  = (0, 1)\n",
    "sigma = 0.1                                   \n",
    "deg_range = range(10)\n",
    "\n",
    "# generate data\n",
    "x_train, y_train = generate_data(n, a, b , sigma)\n",
    "x_test, y_test = generate_data(n_test, a, b , sigma)\n",
    "\n",
    "loss_train = []\n",
    "loss_test = []\n",
    "for deg in deg_range:\n",
    "\n",
    "    w_ols = mod3.ols(x_train, y_train, p=deg)\n",
    "    \n",
    "    loss_train += [np.sqrt(2*loss(x_train, y_train, w_ols))]\n",
    "    loss_test += [np.sqrt(2*loss(x_test, y_test, w_ols))]\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(deg_range, loss_train, label='train')\n",
    "plt.plot(deg_range, loss_test, label='test')\n",
    "plt.title('Sqrt Loss vs polynomial degree')\n",
    "plt.xlabel('Degree')\n",
    "plt.ylabel('Sqrt loss')\n",
    "plt.legend()\n",
    "# plt.ylim([0, 2])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXPECTED OUTPUT**\n",
    "\n",
    "<img src=\"fig/poly_fit_train_test_1.png\" style=\"width: 450px;display:inline;margin:1px\"><img src=\"fig/poly_fit_train_test_2.png\" style=\"width: 450px;display:inline;margin:1px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXERCISE 7**\n",
    "\n",
    "Cross-validation is a technique to compare between models with the available training data, by partitioning the original dataset into training and test sets, and *rotating* this partitioning. In $k$-fold cross-validation, the original dataset is randomly partitioned into k equal-sized subsets, enabling $k$ evaluations of the models under different partitions of train and test sets. The model with best mean performance and lowest standard deviation is selected.\n",
    "\n",
    "**INSTRUCTIONS**\n",
    "* generate a dataset of size $n=2500$, $a=0$, $b=1$ and $\\sigma=0.1$\n",
    "* partition the dataset into $k=10$ partitions\n",
    "* train and test a polynomial of degree $p=4$ to each partitioning of the dataset. For example, train on partitions 1 to 9, and test on partition 10, then train on partitions 2 to 10, and test on partition 1, etc. \n",
    "* plot the loss for each partitioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross-validation\n",
    "\n",
    "# data params\n",
    "n = 2500\n",
    "a, b  = (0, 1)\n",
    "sigma = 0.1                                   \n",
    "k_fold = 10\n",
    "deg = 4\n",
    "    \n",
    "# generate data\n",
    "x_train, y_train = generate_data(n, a, b, sigma)\n",
    "\n",
    "k_size = int(n/k_fold)\n",
    "\n",
    "n_idx = range(n)\n",
    "loss_fold = []\n",
    "for test_idx in [n_idx[i:i+k_size] for i in xrange(0, n, k_size)]:\n",
    "\n",
    "    if len(test_idx)<20:\n",
    "        continue\n",
    "    \n",
    "    train_idx = [item for item in n_idx if item not in test_idx]\n",
    "    \n",
    "    (x_test_fold, y_test_fold) = (x_train[test_idx], y_train[test_idx])\n",
    "    (x_train_fold, y_train_fold) = (x_train[train_idx], y_train[train_idx])\n",
    "    \n",
    "    w_ols = mod3.ols(x_train_fold, y_train_fold, p=deg)\n",
    "    \n",
    "    loss_fold += [loss(x_test_fold, y_test_fold, w_ols)]\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(range(len(loss_fold)), loss_fold)\n",
    "\n",
    "plt.ylim([0, max(loss_fold)*1.2])\n",
    "plt.title('Loss per fold')\n",
    "plt.xlabel('Fold')\n",
    "plt.ylabel('Loss')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXPECTED OUTPUT**\n",
    "\n",
    "<img src=\"fig/poly_fit_loss_fold.png\" style=\"width:50%;height:50%;display:inline;margin:1px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXERCISE 8**\n",
    "\n",
    "\n",
    "Evaluate performance of different polynomial degrees under a small training set, by ploting the mean $\\mu$ and standard deviation $\\sigma$ of the loss (i.e. the rotated test loss).\n",
    "\n",
    "**INSTRUCTIONS**\n",
    "* generate a train set of size $n=100$, $a=0$, $b=1$ and $\\sigma=0.1$\n",
    "* fit polynomials of degree up to $p=10$\n",
    "* evaluate each model with $k=5$ cross-validation\n",
    "* plot the mean $\\mu$ and standard deviation $\\sigma$ for each polynomial degree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model comparison\n",
    "\n",
    "# data params\n",
    "n = 100\n",
    "n_test = 200\n",
    "a, b  = (0, 1)\n",
    "sigma = 0.1                                   \n",
    "deg_range = range(10)\n",
    "k_fold = 5\n",
    "k_size = int(n/k_fold)\n",
    "n_idx = range(n)\n",
    "\n",
    "\n",
    "# generate data\n",
    "x_train, y_train = generate_data(n, a, b , sigma)\n",
    "x_test, y_test = generate_data(n_test, a, b , sigma)\n",
    "\n",
    "loss_test_fold = np.zeros((len(deg_range), k_fold))\n",
    "for fold_idx, test_idx in enumerate([n_idx[i:i+k_size] for i in xrange(0, n, k_size)]):\n",
    "    \n",
    "    if len(test_idx)<20:\n",
    "        loss_test_fold = np.delete(x,(fold_idx), axis=1)\n",
    "        print 'deleted', fold_idx, loss_test_fold.shape\n",
    "        continue\n",
    "        \n",
    "    train_idx = [item for item in n_idx if item not in test_idx]\n",
    "    \n",
    "    (x_test_fold, y_test_fold) = (x_train[test_idx], y_train[test_idx])\n",
    "    (x_train_fold, y_train_fold) = (x_train[train_idx], y_train[train_idx])\n",
    "    \n",
    "    for deg_idx, deg in enumerate(deg_range):\n",
    "\n",
    "        w_ols = mod3.ols(x_train_fold, y_train_fold, p=deg)\n",
    "\n",
    "        loss_test_fold[deg_idx, fold_idx] = loss(x_test_fold, y_test_fold, w_ols)\n",
    "    \n",
    "loss_test_fold_mean = np.mean(loss_test_fold, axis=1)\n",
    "loss_test_fold_std = np.std(loss_test_fold, axis=1)\n",
    "\n",
    "plt.figure()\n",
    "plt.fill_between(deg_range, loss_test_fold_mean - loss_test_fold_std,\n",
    "                 loss_test_fold_mean + loss_test_fold_std, color='C1', label='sqrt loss $\\mu\\pm\\sigma$')\n",
    "plt.plot(deg_range, loss_test_fold_mean, label='sqrt loss $\\mu$')\n",
    "plt.plot(deg_range, loss_test_fold_mean, 'ko')\n",
    "\n",
    "plt.title('Sqrt Loss vs polynomial degree')\n",
    "plt.xlabel('Degree')\n",
    "plt.ylabel('Sqrt loss')\n",
    "plt.legend()\n",
    "# plt.ylim([0,0.03])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXPECTED OUTPUT**\n",
    "\n",
    "<img src=\"fig/poly_fit_deg_1.png\" style=\"width: 450px;display:inline;margin:1px\"><img src=\"fig/poly_fit_deg_2.png\" style=\"width: 450px;display:inline;margin:1px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXERCISE 9**\n",
    "\n",
    "High degree polynomials have a tendency to wiggle very strongly when fitting small datasets. This is a sign of the model *overfitting* the data. Visualize this effect by varying dataset size $n$, and plotting the polynomial fit and weights.\n",
    "\n",
    "**INSTRUCTIONS**\n",
    "* generate diferent train sets of size $n\\in\\{10, 100, 1000\\}$, $a=0$, $b=1$ and $\\sigma=0.1$\n",
    "* fit a polynomial of degree $p=20$ to each train set\n",
    "* plot the polynomial fit for each dataset size $n$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# regularization by dataset size\n",
    "\n",
    "# data params\n",
    "n_range = (10, 100, 1000)\n",
    "a, b  = (0, 1)\n",
    "sigma = 0.1                                   \n",
    "deg = 20\n",
    "\n",
    "gs = gridspec.GridSpec(3, 2)\n",
    "\n",
    "plt.figure(figsize=(9, 9))\n",
    "\n",
    "for idx, n in enumerate(n_range):\n",
    "    \n",
    "    x_train, y_train = generate_data(n, a, b , sigma)\n",
    "\n",
    "    w_ols = mod3.ols(x_train, y_train, p=deg)\n",
    "\n",
    "    plt.subplot(gs[idx, 0])\n",
    "    title='Dataset n='+str(n)\n",
    "    mod3.plot_data(x_train, y_train, title=title)\n",
    "    plt.plot(x_range_ext, g(x_range_ext, w_ols), alpha=0.6)\n",
    "    plt.xlim([x_range_ext[0], x_range_ext[-1]])\n",
    "    plt.ylim([-2, 2])\n",
    "    \n",
    "    plt.subplot(gs[idx, 1])\n",
    "    mod3.plot_bars(w_ols, title='Weights', ax_labels=('idx', 'w'))\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXPECTED OUTPUT**\n",
    "\n",
    "<img src=\"fig/poly_fit_reg_data.png\" style=\"width:100%;height:100%;display:inline;margin:1px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXERCISE 10**\n",
    "\n",
    "The plots of weight values from the previous question do not clearly show that dataset size has a regularizing effect on the norm of the weigths $\\mathbf{w}$. Investigate this question by plotting the mean $\\mu$ and standard deviation $\\sigma$ of the L2 norm of the weights, over $N$ relizations of the sampling and fitting process.\n",
    "\n",
    "**INSTRUCTIONS**\n",
    "* generate $N$ train sets of sizes $n\\in\\{10, 100, 1000, 10000\\}$, $a=0$, $b=1$ and $\\sigma=0.1$\n",
    "* fit a polynomial of degree $p=20$ to each train set\n",
    "* evaluate the mean $\\mu\\left(\\|W\\|^2\\right)$ and standard deviation $\\sigma\\left(\\|W\\|^2\\right)$ of the L2 norm for each train set size $n$\n",
    "* use function mod3.plot_bars to plot bars representing weight values\n",
    "* the L2 norm is available in Numpy as `np.linalg.norm`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# regularization by dataset size - ensemble stats over many realizations\n",
    "\n",
    "# data params\n",
    "n_range = (10, 100, 1000, 10000)\n",
    "a, b  = (0, 1)\n",
    "sigma = 0.1                                   \n",
    "deg = 15\n",
    "N = 200\n",
    "\n",
    "norm = np.zeros((len(n_range), N))\n",
    "for idx, n in enumerate(n_range):\n",
    "    \n",
    "    for idx2 in enumerate(range(N)):\n",
    "        x_train, y_train = generate_data(n, a, b , sigma)\n",
    "        w_ols = mod3.ols(x_train, y_train, p=deg)\n",
    "        norm[idx, idx2]= np.linalg.norm(w_ols)\n",
    "\n",
    "norm_mean = np.mean(norm, axis=1)\n",
    "norm_std = np.std(norm, axis=1)\n",
    "\n",
    "plt.figure()\n",
    "plt.fill_between(n_range, norm_mean - norm_std,\n",
    "                 norm_mean + norm_std, color='C1', label='$\\|\\|W\\|\\|^2$ $\\mu\\pm\\sigma$')\n",
    "plt.plot(n_range, norm_mean, label='$\\|\\|W\\|\\|^2$ $\\mu$')\n",
    "plt.plot(n_range, norm_mean, 'ko')\n",
    "\n",
    "plt.gca().set_xscale('log')\n",
    "plt.gca().set_yscale('log')\n",
    "\n",
    "plt.title('Weights L2 norm vs dataset size')\n",
    "plt.xlabel('N')\n",
    "plt.ylabel('$\\|\\|W\\|\\|^2$')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXPECTED OUTPUT**\n",
    "\n",
    "<img src=\"fig/poly_fit_reg_data_stats.png\" style=\"width:50%;height:50%;display:inline;margin:1px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXERCISE 11**\n",
    "\n",
    "Extend the range of $x$ to include a few full periods of the sinusoidal and examine its effect in optimal value(s) $p^*$ under a small training set.\n",
    "\n",
    "\n",
    "**INSTRUCTIONS**\n",
    "* generate a train set of size $n=100$, $a=-1$, $b=2$ and $\\sigma=0.3$\n",
    "* repeat the analysis from *Exercise 4*\n",
    "* repeat the analysis from *Exercise 8*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# polynomial regression\n",
    "\n",
    "reload(mod3)\n",
    "\n",
    "# data params\n",
    "n = 100\n",
    "a, b = (-1, 2)\n",
    "sigma = 0.1\n",
    "deg = 7\n",
    "\n",
    "# generate data\n",
    "x, y = generate_data(n, a, b , sigma)\n",
    "\n",
    "w_ols = mod3.ols(x, y, p=deg)\n",
    "\n",
    "x_range_bis = np.linspace(a-0.1*(b-a), b+0.1*(b-a), num=500)\n",
    "x_range_ext_bis = np.linspace(a-0.5*(b-a), b+0.5*(b-a), num=500)\n",
    "\n",
    "gs = gridspec.GridSpec(1, 3)\n",
    "\n",
    "plt.figure(figsize=(9, 3))\n",
    "\n",
    "plt.subplot(gs[0])\n",
    "plt.plot(x_range_bis, g(x_range_bis, w_ols), alpha=0.8, label='poly'+str())\n",
    "mod3.plot_data(x, y, title='Polynomial fit')\n",
    "\n",
    "plt.subplot(gs[1])\n",
    "plt.plot(x_range_ext_bis, f(x_range_ext_bis), 'C2', alpha=0.8)\n",
    "plt.plot(x_range_ext_bis, g(x_range_ext_bis, w_ols), alpha=0.8, label='poly'+str())\n",
    "mod3.plot_data(x, y, title='Polynomial fit')\n",
    "plt.xlim([x_range_ext_bis[0], x_range_ext_bis[-1]])\n",
    "\n",
    "plt.subplot(gs[2])\n",
    "mod3.plot_bars(w_ols, title='Weights', ax_labels=('idx', 'w'))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# data params\n",
    "n = 100\n",
    "n_test = 200\n",
    "a, b = (-1, 2)\n",
    "sigma = 0.1                                 \n",
    "deg_range = range(20)\n",
    "k_fold = 5\n",
    "k_size = int(n/k_fold)\n",
    "n_idx = range(n)\n",
    "\n",
    "# generate data\n",
    "# x_train, y_train = generate_data(n, a, b , sigma)\n",
    "# x_test, y_test = generate_data(n_test, a, b , sigma)\n",
    "\n",
    "loss_test_fold = np.zeros((len(deg_range), k_fold))\n",
    "for fold_idx, test_idx in enumerate([n_idx[i:i+k_size] for i in xrange(0, n, k_size)]):\n",
    "    \n",
    "    if len(test_idx)<20:\n",
    "        loss_test_fold = np.delete(x,(fold_idx), axis=1)\n",
    "        print 'deleted', fold_idx, loss_test_fold.shape\n",
    "        continue\n",
    "        \n",
    "    train_idx = [item for item in n_idx if item not in test_idx]\n",
    "    \n",
    "    (x_test_fold, y_test_fold) = (x_train[test_idx], y_train[test_idx])\n",
    "    (x_train_fold, y_train_fold) = (x_train[train_idx], y_train[train_idx])\n",
    "    \n",
    "    for deg_idx, deg in enumerate(deg_range):\n",
    "\n",
    "        w_ols = mod3.ols(x_train_fold, y_train_fold, p=deg)\n",
    "\n",
    "        loss_test_fold[deg_idx, fold_idx] = loss(x_test_fold, y_test_fold, w_ols)\n",
    "    \n",
    "loss_test_fold_mean = np.mean(loss_test_fold, axis=1)\n",
    "loss_test_fold_std = np.std(loss_test_fold, axis=1)\n",
    "\n",
    "plt.figure()\n",
    "plt.fill_between(deg_range, loss_test_fold_mean, \n",
    "                 loss_test_fold_mean + loss_test_fold_std,\n",
    "                 color='C1', label='sqrt loss $\\mu\\pm\\sigma$')\n",
    "plt.fill_between(deg_range, loss_test_fold_mean, \n",
    "                 loss_test_fold_mean - loss_test_fold_std,\n",
    "                 where=(loss_test_fold_mean - loss_test_fold_std)>0,\n",
    "                 color='C1')\n",
    "plt.plot(deg_range, loss_test_fold_mean, label='sqrt loss $\\mu$')\n",
    "plt.plot(deg_range, loss_test_fold_mean, 'ko')\n",
    "\n",
    "plt.title('Sqrt Loss vs polynomial degree')\n",
    "plt.xlabel('Degree')\n",
    "plt.ylabel('Sqrt loss')\n",
    "plt.legend()\n",
    "# plt.ylim([0,0.1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXPECTED OUTPUT**\n",
    "\n",
    "<img src=\"fig/poly_fit_range.png\" style=\"width:90%;height:90%;display:inline;margin:1px\">\n",
    "<img src=\"fig/poly_fit_deg_extended_1.png\" style=\"width:450px;height:450px;display:inline;margin:1px\">\n",
    "<img src=\"fig/poly_fit_deg_extended_2.png\" style=\"width:450px;height:450px;display:inline;margin:1px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXTENDED EXERCISE 1**\n",
    "\n",
    "Why is $\\sigma\\left(\\|W\\|^2\\right)$ from *Exercise 10* not symetric? Explain the scaling of $\\mu\\left(\\|W\\|^2\\right)$ with $N$. How does its peak value scale with $p$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXTENDED EXERCISE 2**\n",
    "\n",
    "Add a penalty term to $C(\\mathcal{D},\\mathbf{w})$ in order to discourage large weight values, i.e. to regularize the weights.\n",
    "\n",
    "$$C(\\mathcal{D},\\mathbf{w})_\\lambda=\\frac{1}{2n}\\sum _{i=1}^{n}(y_{i}-\\mathbf{x}_{i}^{T}\\mathbf{w})^{2}+\\lambda\\|W\\|^2 ,\n",
    "$$\n",
    "\n",
    "Derive the maximum likelihood solution $\\mathbf{w}*$ under $C(\\mathcal{D},\\mathbf{w})_\\lambda$. What is the effect of including/excluding $w_0$ in the penalty term? How does $\\mu\\left(\\|W\\|^2\\right)$ evolve with $\\lambda$? The Bishop is your friend."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXTENDED EXERCISE 3**\n",
    "\n",
    "Examine the effect of extending the range of $x$ in regards to optimal value $p^*$ by using ensemble statistics. Plot the scaling do of $p^*$ with the range of $x$. Relate it to minimal polynomial degree that it would take to approximate the function with a truncated Taylor expansion of degree $p$.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
