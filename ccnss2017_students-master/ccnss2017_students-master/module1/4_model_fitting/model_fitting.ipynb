{"nbformat_minor": 1, "cells": [{"execution_count": 4, "cell_type": "code", "metadata": {"collapsed": true}, "outputs": [], "source": []}, {"source": ["# Introduction\n", "\n", "In this tutorial we will \"fit\" the drift diffusion model to experimental reaction time data, that is we will learn how to choose the parameters of the model such that it \"optimally\" matches the data. In general, a model is given by a parameterized function specifying and constraining a family of allowed dependencies between independent and dependent variables. Also, in general, this functional form is not known *a priori*, and one therefore needs to compare several reasonable candidate models and select the most appropriate one given the data."], "cell_type": "markdown", "metadata": {}}, {"source": ["## Prerequisites\n", "- psychophysics reaction time distributions\n", "- drift diffusion model\n", "- basic math and statistics\n", "- basic Python"], "cell_type": "markdown", "metadata": {}}, {"source": ["## Objectives\n", "- learn about and apply Ordinary Least Squares & Maximum Likelihood regression\n", "- fit drift diffusion model to simulated and experimental reaction-time data\n", "- perform model selection given a set of candidate models"], "cell_type": "markdown", "metadata": {}}, {"source": ["# General methodology\n", "We will start with an example of the most common fitting methodology."], "cell_type": "markdown", "metadata": {}}, {"source": ["## Ordinary Least Squares (OLS) Regression\n", "\n", "We have data pairs $(x_i,y_i)$ that we think are linearly related but we are not sure what the slope or intercept is that best characterizes this relationship. To find this, we fit the data with a linear model\n", "\n", "$\\hat{y}_i = \\beta_1 x_i + \\beta_0$\n", "\n", "and estimate the best fitting parameters by minimizing the mean squared error (MSE)\n", "\n", "$\\sum_i(y_i - \\hat{y}_i)^2 = \\sum_i (y_i-\\beta_1 x_i-\\beta_0)^2$\n", "\n", "For the case of linear regression, there is actually an analytical solution:\n", "\n", "$\\beta_1 = \\frac{cov(x,y)}{var(x)}$\n", "\n", "$\\beta_0 = \\bar{y} - \\hat{\\beta_1} x$\n", "\n", "but we will use a more general optimization library to start familiarizing ourselves with these tools."], "cell_type": "markdown", "metadata": {}}, {"source": ["### Exercise 1: OLS\n", "\n", "1. Generate N = 100 data pairs $(x_i,y_i)$ using a linear model with normally distributed noise $\\epsilon$ and your choice of slope $\\beta_1$ and intercept $\\beta_0$ parameters.\n", "2. Calculate the analytical estimate for the OLS regression\n", "3. Write a function that returns the mean square error (MSE) for any parameter values.\n", "4. Use an optimization library to numerically find the parameters that minimize the MSE and compare these to the true parameters of the generative model\n", "5. Plot the data, as well as the anlytical and numerical OLS estimate\n", "\n", "Tip: Use the `minimize` function from the `scipy.optimize` module"], "cell_type": "markdown", "metadata": {}}, {"execution_count": 2, "cell_type": "code", "metadata": {}, "outputs": [], "source": []}, {"source": ["## Maximum Likelihood Estimation (MLE)\n", "\n", "We can also fit the model to the data using Maximum Likelihood methods. To do this, we take the following generative model for the data:\n", "\n", "$\\hat{y}_i = \\beta_1 x_i + \\beta_0 + \\epsilon$\n", "\n", "where $\\epsilon\\sim \\mathcal{N}(0,\\sigma^2)$ is a normally distributed random variable with mean 0 and variance $\\sigma^2$ and $y\\sim \\mathcal{N}(\\beta_1 x+\\beta_0,\\sigma^2)$.\n", "\n", "The probability distribution of $y$ given $x$ is then given by: \n", "\n", "\\begin{eqnarray}\n", "\\\\\n", "p(y_i|x_i,\\beta_1,\\beta_0) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp[-(y_i-\\beta_1 x_i-\\beta_0)^2/(2\\sigma^2)]\n", "\\end{eqnarray}\n", "\n", "For a pair $(x_i,y_i)$, the log likelihood of observation $y_i$ is \n", "\\begin{eqnarray}\n", "\\log p(y_i|x_i,a,b)\n", "= \\log [\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp[-(y_i-\\beta_1 x_i-\\beta_0)^2/(2\\sigma^2)]] \\\\\n", "= \\log [\\frac{1}{\\sqrt{2\\pi\\sigma^2}}] -(y_i-\\beta_1 x_i-\\beta_0)^2/(2\\sigma^2)\n", "\\end{eqnarray}\n", "\n", "That is: When $\\epsilon$ is normally distributed, maximizing the total log likelihood of the data is equivalent to minimizing the mean squared error.\n"], "cell_type": "markdown", "metadata": {}}, {"source": ["### Exercise: MLE\n", "\n", "1. Generate N = 100 data pairs (y,x) using a linear model with normally distributed noise $\\epsilon$ and your choice of slope $\\beta_1$ and intercept $\\beta_0$ parameters.\n", "2. Write a function that returns the total negative log likelihood for any parameter values.\n", "3. Use an optimization library to numerically find the parameters that minimize the negative log likelihood (or equivalently, maximize the log likelihood of the data given the model).\n", "\n", "Tip: Use the `minimize` function from the `scipy.optimize` module.\n", "\n", "Hint: You can add a very small number to your likelihood to make sure you are not taking $\\log(0)$"], "cell_type": "markdown", "metadata": {}}, {"execution_count": 3, "cell_type": "code", "metadata": {}, "outputs": [], "source": []}, {"source": ["### Exercise: Likelihood heat map\n", "1. Plot the likelihood heat map as a function of $\\beta_1$ and $\\beta_0$"], "cell_type": "markdown", "metadata": {}}, {"execution_count": 4, "cell_type": "code", "metadata": {}, "outputs": [], "source": []}, {"source": ["**Expected Output**\n", "(with random seed == 1)\n", "<img src=\"fig/lin_ll_surface.png\">"], "cell_type": "markdown", "metadata": {}}, {"source": ["# Application: Fitting the DDM\n", "\n", "Now that we have looked at model fitting for a simple case, we can try to fit the DDM to the monkey reaction time data from last class in order to find the best fitting mean, noise and boundary parameters.\n", "\n", "As a first step, we will test our ability to recover the parameters of the model on simulated data for which we set the parameters.\n", "\n", "Once we are convinced that we can recover the parameters on simulated data, we will fit the model to the experimental monkey data.\n"], "cell_type": "markdown", "metadata": {}}, {"source": ["## Excercise: Histogram of simulated RTs\n", "1. Generate 5000 trials of simulated RT data using the constant bound DDM function ($\\mu = 0.0015, \\sigma = 0.05, B = 1$)\n", "2. Plot the simulated RT data using your plot function from yesterday\n", "\n", "Tip: We will import a data simulation function `sim_DDM_constant`, a plotting function `plot_rt_distribution` and a function that computes the analytic DDM and returns the (RG) `analytic_ddm` from the module ddm. You can use this module or you can use your own based on the work from the last tutorial."], "cell_type": "markdown", "metadata": {}}, {"execution_count": 5, "cell_type": "code", "metadata": {"collapsed": true}, "outputs": [], "source": []}, {"execution_count": 6, "cell_type": "code", "metadata": {}, "outputs": [], "source": []}, {"source": ["*Expected output*\n", "<img src=\"fig/RT_simulated_cb.png\">"], "cell_type": "markdown", "metadata": {}}, {"source": ["## Exercise: Likelihood from analytic DDM\n", "\n", "1. Implement the following function\n", "```\n", "def get_nll_ddm(parameter, sigma, rts, corrects):\n", "    '''\n", "    Determines the negative loglikelihood of the analytical DDM\n", "    \n", "    Parameters\n", "    ----------\n", "    parameter : array_like of float\n", "        length 2: 1st entry is mu (drift rate), 2nd is B (boundary)\n", "        Note: we pack mu and B in one parameter because we want to\n", "        make it compatible for later use with sp.optimize.minimize\n", "    sigma : float\n", "        DDM standard deviation\n", "    rts : array_like of floats\n", "        reaction times for which the likelihood will be evaluated\n", "    corrects: array_like of bools, same length as rts\n", "        indicates for each rt if it was a correct trial\n", "        \n", "    Returns\n", "    -------\n", "    nll : float\n", "        negative log-likelihood\n", "    '''\n", "```\n", "\n", "1. Use the `analytic_ddm` function from the module `ddm` to calculate the log likelihood for a correct trial where RT is $500ms$, $\\mu=0.0015$ and $B=1$.\n", "2. What's the log-likelihood for an incorrect trial with otherwise identical parameters?\n", "\n", "4. What's the analytical log-likelihood of the decision-variable trajectory from the previous exercise?"], "cell_type": "markdown", "metadata": {}}, {"execution_count": 7, "cell_type": "code", "metadata": {"collapsed": true}, "outputs": [], "source": []}, {"execution_count": 8, "cell_type": "code", "metadata": {}, "outputs": [], "source": []}, {"execution_count": 9, "cell_type": "code", "metadata": {}, "outputs": [], "source": []}, {"source": ["## Exercise: Fit DDM to simulated monkey reaction time distribution"], "cell_type": "markdown", "metadata": {}}, {"source": ["Once you are able to evaluate your likelihood function at various parameter values, it's time to fit the simulated data. The goal here is to pass the negative log likelihood to an optimizer that will find the parameters to minimize the total negative log likelihood."], "cell_type": "markdown", "metadata": {}}, {"source": ["Note that optimizers tend to work better when parameters have the same order of magnitude. Also, the optimization function that we are going to use, `scipy.optimize.minimize`, requires that all parameters that are optimized over are packed into a vector and that this vector is the first argument of the objective function.\n", "\n", "- Write a wrapper function that's exactly like `get_nll_ddm`, except that it takes as first argument the vector $(1000 \\mu,B)$.\n", "\n", "Remember that this will mean rescaling the parameters returned by the optimizer in future exercises!"], "cell_type": "markdown", "metadata": {}}, {"execution_count": 10, "cell_type": "code", "metadata": {"collapsed": true}, "outputs": [], "source": []}, {"source": ["2. Use the optimizer `minimize` on your negative log likelihood function to maximize the log likelihood of the simulated data. Again $\\sigma$ will be fixed at 0.05. \n", "3. Is the optimization succesful? If yes, you should see \"message: 'Optimization terminated successfully.'\" in the output. If not, consider using a bounded optimization (check out the bounds input to the function and use method 'SLSQP'). $mu$ and $B$ should be positive.\n", "4. Compare the simulated data with the fitted distribution. To do so, use the analytic_ddm function with the fitted parameter value."], "cell_type": "markdown", "metadata": {}}, {"execution_count": 11, "cell_type": "code", "metadata": {}, "outputs": [], "source": []}, {"source": ["*Expected output*\n", "<img src=\"fig/RT_sim_fits.png\">"], "cell_type": "markdown", "metadata": {}}, {"source": ["## Exercise: Fit DDM to experimental monkey RT data\n", "Now we use the same approach to fit the monkey data for the coherence 0.064.\n", "\n", "1. Plot the distribution of the monkey data and compare it with the distribution of the simulated data. Do you notice any difference?\n", "2. Fit the monkey data for coherence 0.064 using the same approach as before. How is the fit?"], "cell_type": "markdown", "metadata": {}}, {"execution_count": 12, "cell_type": "code", "metadata": {"collapsed": true}, "outputs": [], "source": []}, {"execution_count": 13, "cell_type": "code", "metadata": {}, "outputs": [], "source": []}, {"source": ["*Expected output*\n", "<img src=\"fig/RT_simulated_cb_bins.png\">\n", "<img src=\"fig/RT_monkey_data.png\">"], "cell_type": "markdown", "metadata": {}}, {"execution_count": 14, "cell_type": "code", "metadata": {}, "outputs": [], "source": []}, {"source": ["## Exercise: DDM with non-decision time\n", "We can potentially improve the fit by having a non-decision time, which is a constant added to all reaction time.\n", "\n", "1. Calculate the negative loglikelihood of the monkey data with a non-decision time $t_{nd}$.\n", "2. Maximize the log likelihood with a model of three parameters $\\mu,B,t_{nd}$."], "cell_type": "markdown", "metadata": {}}, {"execution_count": 15, "cell_type": "code", "metadata": {}, "outputs": [], "source": []}, {"source": ["# Model Comparison\n", "We will use the $BIC$ (Bayesian Information Criterion) to compare models:\n", "\\begin{eqnarray}\n", "-2 \\log p(M|y) \\approx -2\\ln(L) + k\\ln(n) \\equiv BIC\n", "\\end{eqnarray}\n", "where \n", "- $M$ is the model under consideration, \n", "- $L$ the likelihood for model $M$,\n", "- $y$ the observed data, \n", "- $k$ the number of free parameters, \n", "- $n$ the number of data points (observations)\n", "\n", "and the approximation holds for large $n$.\n", "\n", "The $BIC$ penalizes more complex models with more parameters. Specifically, in our context, the BIC penalizes the non-decision time model for its extra parameter.\n", "\n", "Note that a lower BIC is better and in general a difference of BIC 10 or more is good evidence for the model with the lower BIC.\n", "\n", "***Reference***\n", "\n", "Wit et al. \u2018All models are wrong...\u2019: an introduction to model uncertainty. Statistica Neederlandia (2012)."], "cell_type": "markdown", "metadata": {}}, {"source": ["## Exercise: BIC\n", "\n", "1. Compare the BIC of the two models\n", "\n", "2. Which model has the smaller BIC (the smaller the better)?"], "cell_type": "markdown", "metadata": {}}, {"execution_count": 16, "cell_type": "code", "metadata": {}, "outputs": [], "source": []}, {"source": ["# [Optional] Exercise: Generalizing ML fitting\n", "\n", "Above we derived for a linear model function $y$ that when the measurment error $\\epsilon$ is normally distributed, then maximizing the total log likelihood of the data is equivalent to minimizing the mean squared error.\n", "\n", "- Now assume your data $\\{{x_i}, {y_i}\\}$ can be described by an arbitrary model function $f(x, \\theta$) where $\\theta$ represents the function's parameters that you want to recover. Moreover, assume that you know that the measurement errors $\\epsilon_i = y_i - f(x_i, \\theta)$ are not normally distributed but follow an arbitrary probability distribution $p$. Derive the likelihood of $\\theta$ given the data under these assumptions.\n", "- Pick a function (e.g. a difference between two exponentials, or a polynomial multiplied by an exponential) and a probability distribution $p$ and fit the RT data. Compare."], "cell_type": "markdown", "metadata": {}}], "metadata": {"kernelspec": {"display_name": "Python [default]", "name": "python2", "language": "python"}, "toc": {"toc_window_display": true, "toc_section_display": "block", "colors": {"hover_highlight": "#DAA520", "running_highlight": "#FF0000", "selected_highlight": "#FFD700"}, "navigate_menu": false, "moveMenuLeft": true, "threshold": 4, "number_sections": true, "sideBar": true, "toc_position": {"height": "856px", "width": "212px", "top": "106px", "right": "auto", "left": "0px"}, "toc_cell": false}, "language_info": {"mimetype": "text/x-python", "nbconvert_exporter": "python", "name": "python", "file_extension": ".py", "version": "2.7.13", "pygments_lexer": "ipython2", "codemirror_mode": {"version": 2, "name": "ipython"}}, "anaconda-cloud": {}}, "nbformat": 4}