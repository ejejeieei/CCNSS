{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import scipy \n",
    "import scipy.stats as sts\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "s_vi=25\n",
    "s_ve=35\n",
    "g_vi=1*80\n",
    "g_ve=1*150\n",
    "tuning_curve_disc=np.linspace(0,180,30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 1\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In the first tutorial of Module 3, we will implement a simple probabilistic population code, performing Bayes-optimal multisensory integration given some simple assumptions. We will encode the heading direction of a subject in two neural populations with Poisson variability, then integrate the information from these populations to calculate the posterior through a simple application of Bayes' rule.\n",
    "\n",
    "\n",
    "## Exercise 1\n",
    "First we will simulate population responses representing heading direction in the visual and vestibular system. We will assume Bell-shaped (Gaussian) tuning curves, with different gains for each population, and Poisson variability.\n",
    "\n",
    "Write a function to generate, given an angle, the response rates in the visual and vestibular populations, with 30 neurons each. Neurons in each population should have equally spaced preferred stimuli between 0 and 180 degrees, and otherwise identical tuning curves $f_i(s)$. You are welcome to experiment with different values for the gains and for the standard deviation that defines the shape of the tuning curves, or you can use the following:  $g_{vi}=80$, $g_{ve}=150$, and $\\sigma=40$.\n",
    "\n",
    "Then write a function that generates spike counts for these populations, using independent Poisson variability.\n",
    "\n",
    "$\\displaystyle p(\\mathbf r \\rvert s)=\\prod_{i=1}^{N} p(r_i \\rvert s)=\\prod_{i=1}^{N}\\frac{e^{-gf_{i}(s)}g \\cdot f_i(s)}{r_i!}$\n",
    "\n",
    "Plot these spike counts for the two populations for an input of 80$^{\\circ}$.\n",
    "\n",
    "<img src=\"spikes_11.png\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rates(s):\n",
    "    vi_rates=g_vi*sts.norm.pdf(s,tuning_curve_disc,s_vi)\n",
    "    ve_rates=g_ve*sts.norm.pdf(s,tuning_curve_disc,s_ve)\n",
    "    \n",
    "    return vi_rates, ve_rates,\n",
    "\n",
    "\n",
    "def counts(s):\n",
    "    \n",
    "    vi_rates,ve_rates=rates(s)  \n",
    "    vi_sp=np.random.poisson(vi_rates)\n",
    "    ve_sp=np.random.poisson(ve_rates)\n",
    "    return vi_sp,ve_sp\n",
    "\n",
    "r_counts=counts(80)\n",
    "visual=plt.plot(tuning_curve_disc,r_counts[0],'o',label='visual')\n",
    "vestibular=plt.plot(tuning_curve_disc,r_counts[1],'o',label='vestibular')\n",
    "combined=plt.plot(tuning_curve_disc,r_counts[1]+r_counts[0],'o',label='combined')\n",
    "plt.ylabel('# of spikes')\n",
    "plt.xlabel('Preferred angle for neuron')\n",
    "plt.yticks(range(max(r_counts[1]+r_counts[0])+2))\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2\n",
    "Write a function to calculate the (discretized) posterior over possible heading directions: \n",
    "\\begin{eqnarray}\n",
    "p(s\\rvert \\mathbf r) \\propto exp\\Big( \\sum_{i}\\big( -g \\cdot f_i(s)+r_i \\cdot \\log (g\\cdot f_i(s)\\big) \\Big),\n",
    "\\end{eqnarray}\n",
    "separately for each of our neural populations. Then perform optimal cue integration by adding up the activity from the two populations $\\mathbf r_{com} =\\mathbf r_{ve}+\\mathbf r_{vi}$, and use it to calculate the joint posterior $p(s\\rvert \\mathbf r_{ve},\\mathbf r_{vi})$. To derive the formula for this posterior, simply note that because of conditional independence given s, $p(s\\rvert \\mathbf r_{ve},\\mathbf r_{vi})=p(s\\rvert \\mathbf r_{vi})p(s\\rvert \\mathbf r_{ve})$\n",
    "\n",
    "Use a discretization of at least 100 points for the range of 0$^{\\circ}$ to 180$^{\\circ}$ of potential heading directions.\n",
    "\n",
    "Plot the resulting posteriors. Also estimate (numerically) the variance of each posterior distribution, and confirm the analytical relationship $\\frac{1}{\\sigma_{com}^2}=\\frac{1}{\\sigma_{ve}^2}+\\frac{1}{\\sigma_{vi}^2}$.\n",
    "\n",
    "<img src=\"posteriors_2.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_posterior(s_input, trial):\n",
    "    r_counts=counts(s_input)\n",
    "   \n",
    "    post_len=180\n",
    "    poterior=np.zeros(post_len)\n",
    "    ss=np.linspace(0,180,post_len)\n",
    "    log_posterior_com=np.zeros(post_len)\n",
    "    log_posterior_ve=np.zeros(post_len)\n",
    "    log_posterior_vi=np.zeros(post_len)\n",
    "    for sc in xrange(post_len):\n",
    "        s=ss[sc]\n",
    "        \n",
    "        log_posterior_com[sc]=np.sum(-(rates(s)[0]+rates(s)[1])+r_counts[0]*np.log(rates(s)[0])+r_counts[1]*np.log(rates(s)[1]))\n",
    "        log_posterior_vi[sc]=np.sum(-rates(s)[0]+r_counts[0]*np.log(rates(s)[0]))\n",
    "        log_posterior_ve[sc]=np.sum(-rates(s)[1]+r_counts[1]*np.log(rates(s)[1]))\n",
    "    \n",
    "    a= [log_posterior_com,log_posterior_ve,log_posterior_vi]\n",
    "    \n",
    "    for kc in xrange(len(a)):\n",
    "        k=a[kc]\n",
    "        k=np.exp(k)/sum(np.exp(k))\n",
    "        a[kc]=k\n",
    "    if trial%100==0:\n",
    "       \n",
    "        plt.plot(a[2],'o',label='visual')\n",
    "        plt.plot(a[1],'o',label='vestibular')\n",
    "        plt.plot(a[0],'o',label='combined')\n",
    "        plt.ylabel('posterior density')\n",
    "        plt.xlabel('Heading direction')\n",
    "        \n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "      \n",
    "    errors=[]\n",
    "    errors2=[]\n",
    "    estimates=[]\n",
    "    for k in enumerate(a):\n",
    "        \n",
    "        \n",
    "        estimate=np.dot(ss,k[1])\n",
    "        error=np.linalg.norm(estimate-s_input)\n",
    "        errors.append(error)\n",
    "        \n",
    "        \n",
    "        estimate2=ss[np.argmax(k[1])]\n",
    "        error2=np.linalg.norm(estimate2-s_input)\n",
    "        errors2.append(error2)\n",
    "       \n",
    "        estimates.append(estimate2)\n",
    "    var=np.zeros(3)\n",
    "    var[0]=np.dot(a[0],np.arange(180)**2)-(np.dot(a[0],np.arange(180)))**2\n",
    "    var[1]=np.dot(a[1],np.arange(180)**2)-(np.dot(a[1],np.arange(180)))**2\n",
    "    var[2]=np.dot(a[2],np.arange(180)**2)-(np.dot(a[2],np.arange(180)))**2\n",
    "    \n",
    "    check_equation=np.dot(np.array([-1,1,1]),var**-1)\n",
    "    \n",
    "    \n",
    "    return errors, errors2, estimates\n",
    "s_input=180*np.random.rand()\n",
    "trial=0\n",
    "get_posterior(s_input, trial)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your function should also return an estimate of the angle, such as the posterior mean or the maximum a posteriori (MAP) estimate (these are equal for a Gaussian posterior, but may differ numerically-which one do you think is better in this case?) for each population given an input, and the corresponding estimation error. Run 100 trials with random input angles and compare the total errors when using the separate and combined populations.\n",
    "\n",
    "Then change your code s.t. the same stimulus is presented repeatedly for a hundred trials, and estimate the variance of your stimulus estimate for each population. Confirm that the same relationship holds for the trial to trial fluctuations of the estimate as for the variances of the single-trial posterior.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cum_errors=np.zeros(3)\n",
    "cum_errors2=np.zeros(3)\n",
    "for trial in xrange(100):\n",
    "    s_input=180*np.random.rand()\n",
    "    errors, errors2,_=get_posterior(s_input, trial)\n",
    "    cum_errors+=errors\n",
    "    cum_errors2+=errors2\n",
    "\n",
    "print cum_errors\n",
    "print cum_errors2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimates=[]\n",
    "s_input=180*np.random.rand()\n",
    "for trial in xrange(100):\n",
    "   \n",
    "    _, _,estimate=get_posterior(s_input, trial)\n",
    "    \n",
    "    estimates.append(estimate)\n",
    "    \n",
    "variances=np.var(np.asarray(estimates),axis=0)\n",
    "print np.dot(variances**-1,np.array([-1,1,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3\n",
    "\n",
    "In real brains, variability is often neither exactly Poisson, nor is it independent between neurons, and tuning curves might not be matched exactly between brain regions and neurons. However, a simple linear scheme for representing a joint posterior without loss of information can still be implemented, if certain conditions on the tuning curves and the noise covariance are met. \n",
    "\n",
    "In particular we will consider the case of Poisson like variability, where the noise is from the exponential family with linear sufficient statistics:\n",
    "\n",
    "$p(\\mathbf r\\rvert s)=\\frac{\\Psi(\\mathbf r)}{\\eta (s)}e^{\\mathbf h(s) \\cdot \\mathbf r}$. (Here $\\eta (s)$ serves to normalize the distribution.)\n",
    "\n",
    "Then it can be shown, that in this case\n",
    "$\\mathbf h^{'}(s)=\\Sigma^{-1}(s)\\cdot\\mathbf f^{'}(s)$\n",
    "\n",
    "In our first example we had $h_i(s)=\\log f_i(s)$, with identical tuning curves for our two input populations, and a diagonal covariance matrix. More generally, the necessary conditions will still be met if the tuning curves can be linearly mapped on to a common basis of tuning functions, s.t.\n",
    "$\\mathbf h_{vi}(s)=\\mathbf W_{vi}\\mathbf H(s)$ and $\\mathbf h_{ve}(s)=\\mathbf W_{ve}\\mathbf H(s)$. \n",
    "\n",
    "Then the linear combination $\\mathbf r_{com} = \\mathbf{W_{vi}^{T}r_{vi}+W_{ve}^{T}r_{ve}}$.\n",
    "\n",
    "Choose a basis H(s) (you could choose e.g. a set of basis functions that is a mix of log Gaussian and log sigmoid functions), and check if you still get a better readout of the combined populations if you rerun the experiments form the previous exercises, using sparse loading weight matrices $\\mathbf W$.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
