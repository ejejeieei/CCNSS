{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "In this tutorial, we will learn how to implement different policies and temporal difference learning algorithms, as well as a hybrid (model building) algorithm, then compare their performance to the dynamic programming algorithms we implemented in the first tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook setup\n",
    "\n",
    "## Instructions\n",
    "\n",
    "- Import numpy, scipy and matplotlib\n",
    "- Configure inline plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "% matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pylab import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial\n",
    "\n",
    "## Learning algorithms and policies\n",
    "\n",
    "__Learning algorithms__:\n",
    "\n",
    "*Sarsa (on-policy)*\n",
    "\n",
    "\\begin{align}\n",
    "Q(s_t,a_t) \\leftarrow Q(s_t,a_t) + \\alpha \\big(r_{t+1} + \\gamma Q(s_{t+1},a_{t+1}) - Q(s_t,a_t)\\big)\n",
    "\\end{align}\n",
    "\n",
    "with temporal discount rate $\\gamma$ and learning rate $\\alpha$.\n",
    "\n",
    "*Q-learning (off-policy)*\n",
    "\n",
    "\\begin{align}\n",
    "Q(s_t,a_t) \\leftarrow Q(s_t,a_t) + \\alpha \\big(r_{t+1} + \\gamma\\max_\\limits{a} Q(s_{t+1},a_{t+1}) - Q(s_t,a_t)\\big)\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "__Policies__:\n",
    "\n",
    "*Epsilon-greedy*\n",
    "\n",
    "\\begin{align}\n",
    "P(a_t|s_t) = \\epsilon \\frac{1}{N_a}  + (1-\\epsilon)1[a_t =\\max_\\limits{a}Q(a_t,s_t)]\n",
    "\\end{align}\n",
    "\n",
    "*Softmax*\n",
    "\n",
    "\\begin{align}\n",
    "P(a_t|s_t) = \\frac{\\exp(Q(a_t,s_t)/\\tau)}{\\Sigma_{i=1}^n \\exp(Q(i)/\\tau)}\n",
    "\\end{align}\n",
    "\n",
    "Please familiarize yourself with the code below, as it will help your write your own code to solve the exercises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import definitions of the environments.\n",
    "import RL_worlds as worlds\n",
    "\n",
    "# Import helper functions for plotting.\n",
    "from plot_util import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def init_state(params):\n",
    "    \"\"\"\n",
    "    Initialize the state at the beginning of an episode.\n",
    "    Args:\n",
    "        params: a dictionary containing the default parameters.\n",
    "    Returns:\n",
    "        an integer corresponding to the initial state.\n",
    "    \"\"\"\n",
    "    if params['environment'].name == 'windy_cliff_grid':\n",
    "        return 0\n",
    "    elif params['environment'].name == 'n_armed_bandit':\n",
    "        return 0\n",
    "    elif params['environment'].name == 'cheese_world':\n",
    "        return 0\n",
    "    elif params['environment'].name == 'cliff_world':\n",
    "        return 0\n",
    "    elif params['environment'].name == 'quentins_world':\n",
    "        return 54\n",
    "\n",
    "def update_state(state, action, params):\n",
    "    \"\"\"\n",
    "    State transition based on world, action and current state.\n",
    "    Args:\n",
    "        state: integer corresponding to the current state.\n",
    "        action: integer corresponding to the action taken.\n",
    "        params: a dictionary containing the default parameters.\n",
    "    Returns:\n",
    "        an integer corresponding to the next state;\n",
    "        an integer corresponding to the reward received.\n",
    "    \"\"\"\n",
    "    next_state, reward = params['environment'].get_outcome(state, action)\n",
    "    return next_state, reward\n",
    "    \n",
    "def call_policy(state, value, params):\n",
    "    \"\"\"\n",
    "    Call a policy to choose actions, given current state and value function.\n",
    "    Args:\n",
    "        state: integer corresponding to the current state.\n",
    "        value: a matrix indexed by state and action.\n",
    "        params: a dictionary containing the default parameters.\n",
    "    Returns:\n",
    "        an integer corresponding action chosen according to the policy.\n",
    "    \"\"\"\n",
    "    # multiple options for policy\n",
    "    if params['policy'] == 'epsilon_greedy':\n",
    "        return epsilon_greedy(state, value, params)\n",
    "    elif params['policy'] == 'softmax':\n",
    "        return softmax(state, value, params)\n",
    "    else: # random policy (if policy not recognized, choose randomly)\n",
    "        return randint(params['environment'].n_actions)\n",
    "\n",
    "def update_value(prev_state, action, reward, state, value, params):\n",
    "    \"\"\"\n",
    "    Update the value function.\n",
    "    Args:\n",
    "        prev_state: an integer corresponding to the previous state.\n",
    "        action: an integer correspoding to action taken.\n",
    "        reward: a float corresponding to the reward received.\n",
    "        state: an integer corresponding to the current state;\n",
    "          should be None if the episode ended.\n",
    "        value: a matrix indexed by state and action.\n",
    "        params: a dictionary containing the default parameters. \n",
    "    Returns:\n",
    "        the updated value function (matrix indexed by state and action).\n",
    "    \"\"\"\n",
    "    if params['learning_rule'] == 'q_learning':\n",
    "        # off policy learning\n",
    "        return q_learning(prev_state, action, reward, state, value, params)\n",
    "    elif params['learning_rule'] == 'sarsa':\n",
    "        # on policy learning\n",
    "        return sarsa(prev_state, action, reward, state, value, params)\n",
    "    else:\n",
    "        print('Learning rule not recognized')\n",
    "\n",
    "def default_params(environment):\n",
    "    \"\"\"\n",
    "    Define the default parameters.\n",
    "    Args:\n",
    "        environment: an object corresponding to the environment.\n",
    "    Returns:\n",
    "        a dictionary containing the default parameters, where the keys\n",
    "            are strings (parameter names).\n",
    "    \"\"\"\n",
    "    params = dict()\n",
    "    params['environment'] = environment\n",
    "    \n",
    "    params['alpha'] = 0.1  # learning rate    \n",
    "    params['beta'] = 10  # inverse temperature    \n",
    "    params['policy'] = 'epsilon_greedy'\n",
    "    params['epsilon'] = 0.05  # epsilon-greedy policy\n",
    "    params['learning_rule'] = 'q_learning'\n",
    "    params['epsilon_decay'] = 0.9\n",
    "    \n",
    "    if environment.name == 'windy_cliff_grid':\n",
    "        params['gamma'] = 0.6  # temporal discount factor\n",
    "    elif environment.name == 'n_armed_bandit':\n",
    "        params['gamma'] = 0.9  # temporal discount factor\n",
    "    elif environment.name == 'cliff_world':\n",
    "        params['gamma'] = 1.0  # no discounting\n",
    "    elif environment.name == 'cheese_world':\n",
    "        params['gamma'] = 0.5  # temporal discount factor\n",
    "    elif environment.name == 'quentins_world':\n",
    "        params['gamma'] = 0.9  # temporal discount factor\n",
    "\n",
    "    return params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Decision Policies\n",
    "\n",
    "1. Write an epsilon-greedy policy function.\n",
    "\n",
    "2. Write a softmax policy function.\n",
    "\n",
    "Tip: both functions should take the current state, the value function and default parameters as input and return an action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Learning Algorithms\n",
    "\n",
    "1. Write a Q-learning (off-policy) algorithm.\n",
    "2. Modify your Q-learning algorithm to obtain a Sarsa (on-policy) algorithm.\n",
    "\n",
    "Tip: both functions should take the previous state, action taken, reward received, value function, current state and default parameters and return the updated value function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3\n",
    "\n",
    "1. Write code that allows you to select a world, a learning algorithm and a decision policy. Run 500 episodes (visits to the world) with learning across episodes. Make sure to set a maximum number of steps per episode (e.g. 1000). Use the functions provided in the plot_util module to:\n",
    "    - Plot the value associated with each action at each state;\n",
    "    - Plot the action corresponding to the maximum value at each state;\n",
    "    - Plot the maximum value in each state;\n",
    "    - Plot the total reward obtained in each episode.\n",
    "2. Experiment with different values for the parameters:\n",
    "    - Pick a range for the learning rate $\\alpha$ and look at how the results change.\n",
    "    - Pick a range for the inverse temperature $\\beta$ (using a softmax policy) and look at how the results change.\n",
    "    - Pick a range for $\\epsilon$ (using an $\\epsilon$-greedy policy) and look at how the results change.\n",
    "    - Pick a range for the temporal discount factor $\\gamma$ and look at how the results change.\n",
    "3. Explore the cliff world with an $\\epsilon$-greedy policy (try $\\epsilon$=0.1) comparing the performance of Q-learning (off-policy) and Sarsa (on-policy). What differences do you notice? What do these differences tell us about on- and off-policy learning?\n",
    "4. Compare your results using Q-learning and Sarsa with those obtained in the previous tutorial. What do you notice about the differences in performance between dynamic programming and TD learning algorithms? What are some of the advantages and disadvantages of each approach?\n",
    "\n",
    "To make sure that your algorithms have been implemented correctly, compare your results to the ones shown below.\n",
    "\n",
    "Cliff world using Q-learning and an $\\epsilon$-greedy policy with $\\epsilon$=0.1 and $\\alpha$=0.3:\n",
    "\n",
    "<img src=\"fig/tutorial2_ex3_qlearning_values.png\",height=\"300\",width=\"300\",align=\"left\">\n",
    "<img src=\"fig/tutorial2_ex3_qlearning_actions.png\",height=\"300\",width=\"300\">\n",
    "<img src=\"fig/tutorial2_ex3_qlearning_maxval.png\",height=\"300\",width=\"300\",align=\"left\">\n",
    "<img src=\"fig/tutorial2_ex3_qlearning_rewards.png\",height=\"300\",width=\"300\">\n",
    "\n",
    "Quentin's world using Sarsa and a softmax policy with $\\beta$=10 and $\\alpha$=0.4:\n",
    "\n",
    "<img src=\"fig/tutorial2_ex3_sarsa_values.png\",height=\"300\",width=\"300\",align=\"left\">\n",
    "<img src=\"fig/tutorial2_ex3_sarsa_actions.png\",height=\"300\",width=\"300\">\n",
    "<img src=\"fig/tutorial2_ex3_sarsa_maxval.png\",height=\"300\",width=\"300\",align=\"left\">\n",
    "<img src=\"fig/tutorial2_ex3_sarsa_rewards.png\",height=\"300\",width=\"300\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4: Dyna-Q\n",
    "\n",
    "1. Implement the Dyna-Q algorithm for a deterministic environment.\n",
    "\n",
    "Tip: the function should take the default parameters, an integer k for the number random updates, and a threshold for the stopping criterium as input and return a value function and deterministic model of the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5\n",
    "\n",
    "1. Write code that allows you to test the performance of Dyna-Q for a selected world and a selected learning rule. Run 500 episodes (visits to the world) with learning across episodes. Make sure to set a maximum number of steps per episode (e.g. 1000). Use the functions provided in the plot_util module to:\n",
    "    - Plot the value associated with each action at each state;\n",
    "    - Plot the action corresponding to the maximum value at each state;\n",
    "    - Plot the maximum value in each state;\n",
    "    - Plot the total reward obtained in each episode.\n",
    "2. Experiment with different values for the parameters:\n",
    "    - Pick a range for the learning rate $\\alpha$ and look at how the results change.\n",
    "    - Pick a range for the temporal discount factor $\\gamma$ and look at how the results change.\n",
    "    - Pick a range for k (number of random updates performed in Dyna-Q) and look at how the results change.\n",
    "3. Compare these results with those obtained for dynamic programming in the previous tutorial, as well as the TD algorithms from this tutorial.\n",
    "\n",
    "To make sure that your algorithm has been implemented correctly, compare your results to the ones shown below.\n",
    "\n",
    "Windy cliff grid using Dyna-Q and an $\\epsilon$-greedy policy with $\\epsilon$=0.05, $\\alpha$=0.5 and $\\gamma$=0.8:\n",
    "\n",
    "<img src=\"fig/tutorial2_ex5_dynaq_values.png\",height=\"300\",width=\"300\",align=\"left\">\n",
    "<img src=\"fig/tutorial2_ex5_dynaq_actions.png\",height=\"300\",width=\"300\">\n",
    "<img src=\"fig/tutorial2_ex5_dynaq_maxval.png\",height=\"300\",width=\"300\",align=\"left\">\n",
    "<img src=\"fig/tutorial2_ex5_dynaq_rewards.png\",height=\"300\",width=\"300\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 6\n",
    "1. We will now compare Q-learning and Dyna-Q to see how each of these algorithms responds to a change in the structure of the environment. We will use a modified version of the windy cliff grid, windy_cliff_grid_2, which is the same as the original except for the location of the doors leading to the two windy rooms.\n",
    "\n",
    "    - For each algorithm, run 500 episodes in the original world. Then, switch to the modified world and run 500 more episodes.\n",
    "    - You can use an $\\epsilon$-greedy policy with $\\epsilon$=0.05 throughout.\n",
    "    - After both sets of episodes, plot the value of each action at each state, the action of maximum value at each state, the maximum value at each state and the total reward obtained in each episode (plot the accumulated rewards only for the 500 episodes in the modified world).\n",
    "    - What do you notice about the difference in each algorithm's performance once the environment has changed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
